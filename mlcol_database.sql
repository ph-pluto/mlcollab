CREATE DATABASE  IF NOT EXISTS `mlcol` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci */ /*!80016 DEFAULT ENCRYPTION='N' */;
USE `mlcol`;
-- MySQL dump 10.13  Distrib 8.0.31, for Win64 (x86_64)
--
-- Host: localhost    Database: mlcol
-- ------------------------------------------------------
-- Server version	8.0.31

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `admins`
--

DROP TABLE IF EXISTS `admins`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `admins` (
  `admin_id` int NOT NULL,
  PRIMARY KEY (`admin_id`),
  CONSTRAINT `admin_id` FOREIGN KEY (`admin_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `admins`
--

LOCK TABLES `admins` WRITE;
/*!40000 ALTER TABLE `admins` DISABLE KEYS */;
INSERT INTO `admins` VALUES (12);
/*!40000 ALTER TABLE `admins` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `devs`
--

DROP TABLE IF EXISTS `devs`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `devs` (
  `dev_id` int NOT NULL,
  PRIMARY KEY (`dev_id`),
  CONSTRAINT `dev_id` FOREIGN KEY (`dev_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `devs`
--

LOCK TABLES `devs` WRITE;
/*!40000 ALTER TABLE `devs` DISABLE KEYS */;
INSERT INTO `devs` VALUES (13);
/*!40000 ALTER TABLE `devs` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `learners`
--

DROP TABLE IF EXISTS `learners`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `learners` (
  `learner_id` int NOT NULL,
  PRIMARY KEY (`learner_id`),
  CONSTRAINT `learner_id` FOREIGN KEY (`learner_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `learners`
--

LOCK TABLES `learners` WRITE;
/*!40000 ALTER TABLE `learners` DISABLE KEYS */;
INSERT INTO `learners` VALUES (13),(14),(15),(16),(17),(18),(19),(20);
/*!40000 ALTER TABLE `learners` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `log_history`
--

DROP TABLE IF EXISTS `log_history`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `log_history` (
  `log_id` int NOT NULL AUTO_INCREMENT,
  `log_user_id` int NOT NULL,
  `activity` varchar(255) NOT NULL,
  `date` date NOT NULL,
  `time` time NOT NULL,
  `details` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
  PRIMARY KEY (`log_id`,`log_user_id`),
  KEY `user_id_idx` (`log_user_id`),
  CONSTRAINT `log_user_id` FOREIGN KEY (`log_user_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=339 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `log_history`
--

LOCK TABLES `log_history` WRITE;
/*!40000 ALTER TABLE `log_history` DISABLE KEYS */;
INSERT INTO `log_history` VALUES (33,12,'Registration','2024-04-07','16:33:05',NULL),(34,13,'Registration','2024-04-07','16:33:33',NULL),(35,13,'Login','2024-04-07','16:35:00',NULL),(36,12,'Login','2024-04-07','16:35:13',NULL),(37,13,'Logout','2024-04-08','13:56:18',NULL),(38,13,'Login','2024-04-08','13:56:21',NULL),(39,12,'Logout','2024-04-08','13:56:26',NULL),(40,12,'Login','2024-04-08','13:56:30',NULL),(41,12,'Logout','2024-04-08','21:24:09',NULL),(42,12,'Login','2024-04-08','21:34:15',NULL),(43,12,'Logout','2024-04-08','21:43:45',NULL),(44,12,'Login','2024-04-08','21:44:14',NULL),(45,13,'Update Profile','2024-04-09','00:45:17',NULL),(46,13,'Update Profile','2024-04-09','00:45:46',NULL),(47,13,'Update Profile','2024-04-09','00:48:46',NULL),(48,13,'Update Profile','2024-04-09','00:51:00',NULL),(49,13,'Update Profile','2024-04-09','00:51:31',NULL),(50,13,'Login','2024-04-09','15:37:14',NULL),(51,13,'Update Profile','2024-04-09','15:38:52',NULL),(52,13,'Update Profile','2024-04-09','15:39:22',NULL),(53,12,'Login','2024-04-09','15:45:30',NULL),(54,13,'Change Password','2024-04-09','16:29:51',NULL),(55,13,'Logout','2024-04-09','16:30:01',NULL),(56,13,'Login','2024-04-09','16:30:19',NULL),(57,13,'Logout','2024-04-09','22:22:02',NULL),(58,13,'Login','2024-04-09','22:22:05',NULL),(59,13,'Logout','2024-04-09','22:27:17',NULL),(60,13,'Login','2024-04-09','22:27:20',NULL),(61,13,'Login','2024-04-10','12:56:34',NULL),(62,12,'Login','2024-04-10','12:56:44',NULL),(63,12,'Update Profile','2024-04-10','15:00:55',NULL),(64,12,'Update Profile','2024-04-10','15:03:49',NULL),(65,13,'Login','2024-04-18','22:15:10',NULL),(66,12,'Login','2024-04-18','22:15:25',NULL),(67,12,'Login','2024-04-19','22:36:25',NULL),(68,13,'Login','2024-04-19','22:36:56',NULL),(69,12,'Update Profile','2024-04-20','15:13:07',NULL),(70,12,'Update Profile','2024-04-20','15:21:03',NULL),(71,12,'Update Profile','2024-04-20','15:21:55',NULL),(72,13,'Update Profile','2024-04-20','15:23:20',NULL),(73,12,'Update Profile','2024-04-20','15:23:28',NULL),(74,12,'Update Profile','2024-04-20','15:23:34',NULL),(75,12,'Update Profile','2024-04-20','15:26:50',NULL),(76,12,'Update Profile','2024-04-20','15:27:10',NULL),(77,12,'Update Profile','2024-04-20','15:28:12',NULL),(78,12,'Update Profile','2024-04-20','15:29:25',NULL),(79,12,'Update Profile','2024-04-20','15:29:54',NULL),(80,12,'Logout','2024-04-20','15:31:17',NULL),(81,12,'Login','2024-04-20','15:31:22',NULL),(82,12,'Update Profile','2024-04-20','15:31:49',NULL),(83,12,'Update Profile','2024-04-20','15:32:36',NULL),(84,13,'Login','2024-04-20','18:11:12',NULL),(85,13,'Update Profile','2024-04-20','18:11:40',NULL),(86,13,'Update Profile','2024-04-20','18:12:12',NULL),(87,12,'Login','2024-04-20','23:53:38',NULL),(88,13,'Login','2024-04-20','23:54:05',NULL),(89,13,'Login','2024-04-21','10:48:32',NULL),(90,12,'Login','2024-04-21','10:57:22',NULL),(91,12,'Login','2024-04-23','11:08:29',NULL),(92,13,'Login','2024-04-23','11:08:44',NULL),(93,13,'Login','2024-04-23','17:09:16',NULL),(94,12,'Login','2024-04-23','20:32:01',NULL),(95,13,'Login','2024-04-24','21:22:12',NULL),(96,13,'Logout','2024-04-24','21:38:14',NULL),(97,13,'Login','2024-04-24','21:38:28',NULL),(98,12,'Login','2024-04-25','00:33:41',NULL),(99,13,'Login','2024-04-25','00:55:47',NULL),(100,12,'Login','2024-04-25','14:48:14',NULL),(101,13,'Login','2024-04-25','14:50:24',NULL),(102,12,'Update Profile','2024-04-25','14:51:21',NULL),(103,13,'Login','2024-04-26','10:25:43',NULL),(104,12,'Login','2024-04-26','10:25:54',NULL),(105,13,'Upload Model','2024-04-26','10:30:04',NULL),(106,13,'Upload Model','2024-04-26','10:31:58',NULL),(107,13,'Delete Model','2024-04-26','10:32:14',NULL),(108,13,'Logout','2024-04-26','15:19:16',NULL),(109,13,'Login','2024-04-26','15:19:22',NULL),(110,12,'Logout','2024-04-26','15:21:00',NULL),(111,12,'Login','2024-04-26','15:21:05',NULL),(112,12,'Logout','2024-04-26','15:47:20',NULL),(113,12,'Login','2024-04-26','15:47:30',NULL),(114,13,'Login','2024-04-26','21:33:43',NULL),(115,13,'Logout','2024-04-26','23:00:30',NULL),(116,13,'Login','2024-04-26','23:01:05',NULL),(117,13,'Login','2024-04-27','13:54:37',NULL),(118,12,'Logout','2024-04-27','13:54:50',NULL),(119,12,'Login','2024-04-27','13:54:55',NULL),(120,13,'Login','2024-04-28','13:36:07',NULL),(121,13,'Login','2024-04-29','00:21:32',NULL),(122,13,'Upload Model','2024-04-29','00:39:17',NULL),(123,13,'Upload Model','2024-04-29','00:41:32',NULL),(124,13,'Upload Model','2024-04-29','00:43:54',NULL),(125,13,'Upload Model','2024-04-29','00:44:16',NULL),(126,13,'Login','2024-04-29','09:26:48',NULL),(127,12,'Login','2024-04-29','14:40:32',NULL),(128,13,'Upload Model','2024-04-29','15:03:35',NULL),(129,13,'Login','2024-04-29','23:27:35',NULL),(130,12,'Login','2024-04-30','00:19:47',NULL),(131,12,'Login','2024-04-30','10:45:33',NULL),(132,13,'Login','2024-04-30','13:18:27',NULL),(133,13,'Logout','2024-04-30','20:59:49',NULL),(134,13,'Login','2024-04-30','20:59:52',NULL),(135,13,'New Comment','2024-04-30','21:34:38',NULL),(136,13,'Update Model','2024-04-30','21:36:41',NULL),(137,13,'Login','2024-05-01','21:57:03',NULL),(138,13,'Login','2024-05-03','01:16:37',NULL),(139,13,'Delete Comment','2024-05-03','02:45:38',NULL),(140,13,'Delete Comment','2024-05-03','02:46:11',NULL),(141,13,'New Comment','2024-05-03','02:56:08',NULL),(142,13,'Delete Comment','2024-05-03','03:00:09',NULL),(143,13,'Login','2024-05-03','21:16:01',NULL),(144,13,'Delete Comment','2024-05-03','21:16:26',NULL),(145,13,'Logout','2024-05-03','23:01:38',NULL),(146,14,'Registration','2024-05-03','23:02:37',NULL),(147,14,'Login','2024-05-03','23:02:53',NULL),(148,12,'Login','2024-05-03','23:03:35',NULL),(149,14,'New Comment','2024-05-03','23:18:24',NULL),(150,14,'New Comment','2024-05-03','23:18:51',NULL),(151,14,'Logout','2024-05-03','23:19:23',NULL),(152,13,'Login','2024-05-03','23:19:28',NULL),(153,13,'Delete Comment','2024-05-03','23:19:48',NULL),(154,13,'Login','2024-05-04','16:46:03',NULL),(155,12,'Login','2024-05-05','00:33:04',NULL),(156,13,'Login','2024-05-05','01:12:38',NULL),(157,13,'Login','2024-05-05','16:26:59',NULL),(158,13,'New Comment','2024-05-05','16:27:31',NULL),(159,13,'Logout','2024-05-05','16:27:58',NULL),(160,13,'Login','2024-05-05','16:28:02',NULL),(161,13,'Logout','2024-05-05','16:28:22',NULL),(162,13,'Login','2024-05-05','16:28:34',NULL),(163,13,'Update Model','2024-05-05','16:29:49',NULL),(164,13,'Delete Model','2024-05-05','16:30:16',NULL),(165,13,'Login','2024-05-05','16:56:24',NULL),(166,13,'New Comment','2024-05-05','16:57:07',NULL),(167,13,'Logout','2024-05-05','17:04:59',NULL),(168,13,'Login','2024-05-05','17:05:04',NULL),(169,13,'Upload Support File','2024-05-05','18:05:10','Model #10, Support file: Jupyter Notebook File for Keras'),(170,13,'Update Model','2024-05-05','18:29:08',NULL),(171,13,'Login','2024-05-05','21:40:11',NULL),(172,13,'Upload Support File','2024-05-05','21:41:28','Model #5, Support file: Variable Index'),(173,13,'Delete Model','2024-05-05','21:41:40',NULL),(174,13,'New Comment','2024-05-05','21:44:48',NULL),(175,13,'Delete Comment','2024-05-05','21:44:54',NULL),(176,13,'New Comment','2024-05-05','21:48:34',NULL),(177,13,'Delete Comment','2024-05-05','21:48:39',NULL),(178,13,'New Comment','2024-05-05','21:50:45',NULL),(179,13,'Delete Comment','2024-05-05','21:50:52',NULL),(180,13,'Upload Support File','2024-05-05','21:51:34','Model #10, Support file: Variable Index'),(181,13,'Delete File','2024-05-05','22:02:03',NULL),(182,13,'Delete File','2024-05-05','22:18:09',NULL),(183,13,'Upload Model','2024-05-05','22:21:44',NULL),(184,13,'Delete Model','2024-05-05','22:21:58',NULL),(185,13,'Upload Support File','2024-05-05','22:24:15','Model #8, Support file: Model_train'),(186,13,'Upload Support File','2024-05-05','22:24:42','Model #8, Support file: Dataset_mt_car'),(187,13,'Login','2024-05-06','15:33:24',NULL),(188,13,'Upload Model','2024-05-06','17:41:06',NULL),(189,13,'Upload Model','2024-05-06','17:49:05',NULL),(190,13,'Login','2024-05-06','22:51:22',NULL),(191,13,'Delete Model','2024-05-07','00:31:54',NULL),(192,13,'Upload Model','2024-05-07','00:38:53',NULL),(193,13,'Upload Model','2024-05-07','00:50:11',NULL),(194,13,'Upload Support File','2024-05-07','00:50:33','Model #16, Support file: fingerprint'),(195,13,'Upload Support File','2024-05-07','00:50:49','Model #16, Support file: keras_metadata'),(196,13,'Upload Support File','2024-05-07','00:51:02','Model #16, Support file: Variable Index'),(197,13,'Upload Support File','2024-05-07','00:51:15','Model #16, Support file: var_data'),(198,13,'Upload Model','2024-05-07','00:59:05',NULL),(199,13,'Logout','2024-05-07','01:22:25',NULL),(200,14,'Login','2024-05-07','01:23:07',NULL),(201,14,'Logout','2024-05-07','01:25:09',NULL),(202,13,'Login','2024-05-07','01:25:14',NULL),(203,13,'New Comment','2024-05-07','01:25:39',NULL),(204,13,'New Comment','2024-05-07','01:26:00',NULL),(205,13,'New Comment','2024-05-07','01:26:21',NULL),(206,13,'New Comment','2024-05-07','01:26:35',NULL),(207,13,'New Comment','2024-05-07','01:26:49',NULL),(208,13,'Delete Comment','2024-05-07','01:27:04',NULL),(209,13,'Delete Comment','2024-05-07','01:27:14',NULL),(210,13,'New Comment','2024-05-07','01:27:52',NULL),(211,13,'Login','2024-05-07','12:22:28',NULL),(212,13,'Delete File','2024-05-07','14:42:05',NULL),(213,12,'Login','2024-05-07','15:02:42',NULL),(214,13,'Login','2024-05-07','16:07:57',NULL),(215,12,'Logout','2024-05-07','16:16:43',NULL),(216,12,'Login','2024-05-07','16:17:21',NULL),(217,13,'Login','2024-05-07','23:32:26',NULL),(218,12,'Login','2024-05-07','23:37:10',NULL),(219,13,'Login','2024-05-08','22:31:25',NULL),(220,12,'Login','2024-05-08','22:33:25',NULL),(221,13,'Logout','2024-05-08','23:20:05',NULL),(222,13,'Login','2024-05-09','02:12:46',NULL),(223,13,'Login','2024-05-09','22:29:56',NULL),(224,12,'Login','2024-05-09','23:10:52',NULL),(225,13,'Upload Model','2024-05-10','01:24:17',NULL),(226,13,'Upload Model','2024-05-10','01:32:23',NULL),(227,13,'Upload Support File','2024-05-10','01:33:04','Model #19, Support file: dataset'),(228,13,'Login','2024-05-10','01:37:45',NULL),(229,13,'Login','2024-05-10','14:12:39',NULL),(230,13,'Login','2024-05-10','20:40:15',NULL),(231,13,'Login','2024-05-11','15:29:52',NULL),(232,13,'Upload Model','2024-05-11','21:11:47',NULL),(233,13,'Update Model','2024-05-11','21:13:40',NULL),(234,13,'Upload Model','2024-05-11','21:14:09',NULL),(235,12,'Login','2024-05-12','00:14:29',NULL),(236,12,'Logout','2024-05-12','00:22:43',NULL),(237,12,'Login','2024-05-12','00:22:47',NULL),(238,12,'Delete Comment','2024-05-12','02:24:56',NULL),(239,12,'Delete Comment','2024-05-12','02:25:45',NULL),(240,12,'Delete Model','2024-05-12','02:32:36',NULL),(241,12,'Delete Comment','2024-05-12','02:32:43',NULL),(242,12,'Login','2024-05-12','14:42:50',NULL),(243,15,'Registration','2024-05-12','15:02:36',NULL),(244,16,'Registration','2024-05-12','15:04:58',NULL),(245,17,'Registration','2024-05-12','15:06:30',NULL),(246,18,'Registration','2024-05-12','15:10:07',NULL),(247,19,'Registration','2024-05-12','15:48:35',NULL),(248,20,'Registration','2024-05-12','15:51:16',NULL),(250,13,'Login','2024-05-12','15:55:47',NULL),(251,12,'Delete Learner Account','2024-05-12','16:50:40',NULL),(254,12,'Delete Post','2024-05-12','17:05:17',NULL),(255,12,'Delete Model','2024-05-12','17:05:47',NULL),(256,13,'Login','2024-05-12','17:18:33',NULL),(257,13,'New Comment','2024-05-12','17:19:46',NULL),(258,12,'Delete Comment','2024-05-12','17:22:50',NULL),(259,13,'Delete Comment','2024-05-12','17:23:31',NULL),(260,12,'Login','2024-05-13','10:10:03',NULL),(261,13,'Login','2024-05-13','10:10:21',NULL),(262,12,'Login','2024-05-13','17:42:10',NULL),(263,12,'Login','2024-05-14','00:43:26',NULL),(264,13,'Login','2024-05-14','00:43:40',NULL),(265,12,'Logout','2024-05-14','00:46:00',NULL),(266,12,'Login','2024-05-14','00:46:04',NULL),(267,13,'Logout','2024-05-14','03:37:04',NULL),(268,13,'Login','2024-05-14','03:37:08',NULL),(269,13,'Login','2024-05-14','11:22:34',NULL),(270,13,'Delete Model','2024-05-14','16:52:11',NULL),(271,13,'Upload Model','2024-05-14','16:53:42',NULL),(272,13,'Update Model','2024-05-14','16:55:23',NULL),(273,13,'Update Model','2024-05-14','16:55:51',NULL),(274,13,'Delete Model','2024-05-14','17:10:28',NULL),(275,13,'Delete Model','2024-05-14','17:10:34',NULL),(276,13,'Delete Model','2024-05-14','17:10:37',NULL),(277,13,'Logout','2024-05-14','17:36:31',NULL),(278,14,'Login','2024-05-14','17:36:35',NULL),(279,12,'Login','2024-05-14','17:37:09',NULL),(280,12,'Delete Post','2024-05-14','17:37:15',NULL),(281,12,'Delete Post','2024-05-14','17:37:18',NULL),(282,12,'Delete Post','2024-05-14','17:37:20',NULL),(283,12,'Delete Post','2024-05-14','17:37:22',NULL),(284,12,'Delete Post','2024-05-14','17:37:24',NULL),(285,12,'Delete Post','2024-05-14','17:37:26',NULL),(286,12,'Delete Post','2024-05-14','17:37:28',NULL),(287,14,'Logout','2024-05-14','17:40:25',NULL),(288,18,'Login','2024-05-14','17:40:58',NULL),(289,18,'Logout','2024-05-14','17:43:23',NULL),(290,17,'Login','2024-05-14','17:43:35',NULL),(291,17,'Logout','2024-05-14','17:45:46',NULL),(292,20,'Login','2024-05-14','17:45:59',NULL),(293,20,'Logout','2024-05-14','17:48:01',NULL),(294,19,'Login','2024-05-14','17:48:19',NULL),(295,12,'Delete Post','2024-05-14','17:50:51',NULL),(296,19,'Logout','2024-05-14','17:51:21',NULL),(297,15,'Login','2024-05-14','17:51:55',NULL),(298,15,'Logout','2024-05-14','17:52:48',NULL),(299,13,'Login','2024-05-14','17:52:53',NULL),(300,13,'Update Profile','2024-05-14','23:14:11',NULL),(301,13,'Update Profile','2024-05-14','23:14:38',NULL),(302,13,'Logout','2024-05-14','23:14:56',NULL),(303,15,'Login','2024-05-14','23:15:03',NULL),(304,15,'Update Profile','2024-05-14','23:22:29',NULL),(305,15,'Logout','2024-05-14','23:23:39',NULL),(306,16,'Login','2024-05-14','23:23:57',NULL),(307,16,'Update Profile','2024-05-14','23:24:14',NULL),(308,16,'Logout','2024-05-14','23:36:52',NULL),(309,13,'Login','2024-05-14','23:36:55',NULL),(310,13,'Logout','2024-05-14','23:49:31',NULL),(311,17,'Login','2024-05-14','23:49:46',NULL),(312,17,'Update Profile','2024-05-14','23:49:58',NULL),(313,17,'Logout','2024-05-14','23:50:14',NULL),(314,13,'Login','2024-05-14','23:50:18',NULL),(315,13,'Login','2024-05-15','00:21:45',NULL),(316,13,'Upload Model','2024-05-15','00:42:41',NULL),(317,13,'Login','2024-05-15','01:09:09',NULL),(318,13,'Delete Model','2024-05-15','03:19:55',NULL),(319,13,'Delete Model','2024-05-15','03:20:02',NULL),(320,13,'Logout','2024-05-15','12:35:01',NULL),(321,13,'Login','2024-05-15','12:35:06',NULL),(322,13,'Logout','2024-05-15','13:28:14',NULL),(323,13,'Login','2024-05-15','23:02:53',NULL),(324,13,'Logout','2024-05-15','23:27:52',NULL),(325,13,'Login','2024-05-15','23:27:55',NULL),(326,13,'Logout','2024-05-15','23:28:11',NULL),(327,14,'Login','2024-05-15','23:28:32',NULL),(328,14,'Update Profile','2024-05-15','23:29:46',NULL),(329,14,'New Comment','2024-05-15','23:30:27',NULL),(330,14,'Logout','2024-05-15','23:30:39',NULL),(331,13,'Login','2024-05-15','23:30:44',NULL),(332,13,'Logout','2024-05-16','10:45:05',NULL),(333,12,'Login','2024-05-16','10:45:11',NULL),(334,13,'Login','2024-05-16','10:45:51',NULL),(335,13,'Login','2024-05-16','23:23:02',NULL),(336,13,'Logout','2024-05-16','23:23:17',NULL),(337,13,'Login','2024-05-16','23:23:20',NULL),(338,12,'Login','2024-05-16','23:23:58',NULL);
/*!40000 ALTER TABLE `log_history` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `model_details`
--

DROP TABLE IF EXISTS `model_details`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `model_details` (
  `model_id` int NOT NULL,
  `description` text,
  `upload_date` date NOT NULL,
  `upload_time` time NOT NULL,
  `parameters` text,
  `file_path` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`model_id`),
  UNIQUE KEY `model_id_UNIQUE` (`model_id`),
  CONSTRAINT `model_id` FOREIGN KEY (`model_id`) REFERENCES `models` (`model_id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `model_details`
--

LOCK TABLES `model_details` WRITE;
/*!40000 ALTER TABLE `model_details` DISABLE KEYS */;
INSERT INTO `model_details` VALUES (3,' The Iris dataset is loaded directly from scikit-learn which is a common practice dataset that includes data on iris flowers and their classifications. Later, the model is deserialized (loaded) from the file and used to make predictions on new data. Moreover, the trained model is then serialized (saved) to a file using Python\'s pickle module.','2024-04-24','22:28:51',NULL,'a47ebd1fc4a079c0b2367d45305aea319d9df7f2.pkl'),(4,'ONNX (Open Neural Network Exchange) is an open standard for representing machine learning models. It aims to provide interoperability between different deep learning frameworks such as PyTorch, TensorFlow, and others.','2024-04-25','00:58:13',NULL,'b60650e879eee84717d40eeafbcdac5531a4a486.onnx'),(7,'Support Vector Machine (SVM) is a machine learning model that has demonstrated superior performance for breast cancer detection. lt divides instances of each class by locating the linear optimum hyperplane after nonlinearly mapping the original data into a high-dimensional feature space. SVMs are used to classify breast cancer tumors as benign or malignant.','2024-04-29','00:39:16',NULL,'12fb41439c934dd2a05de828c7066762119922bb.joblib'),(8,'Simple Test Car Model','2024-04-29','00:41:32',NULL,'0ce9c5f6cf86c20d76356305220c451babbd1386.joblib'),(15,'Test visualize model in pickle format','2024-05-07','00:38:53',NULL,'fb8cead7208ae13482fc73675ce462035d02e32b.pkl'),(19,'ML Model with Feature Importance, Model Parameters and accuracy.','2024-05-10','01:32:23',NULL,'b42a780b653fe572467c584a077f23294cf8b5c8.joblib'),(20,'Let\'s calculate and visualize the feature importance as well. We\'ll use the trained Random Forest classifier to extract the feature importances and visualize them.','2024-05-11','21:11:47',NULL,'c7139162cc538ec78b4b6e01dc015d84fdcc60f6.joblib'),(23,'densenet-timm121','2024-05-15','00:42:41',NULL,'aca94d35c2f614d18f44713b4f9220e16b86ac5c.onnx');
/*!40000 ALTER TABLE `model_details` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `model_files`
--

DROP TABLE IF EXISTS `model_files`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `model_files` (
  `file_id` int NOT NULL AUTO_INCREMENT,
  `file_model_id` int NOT NULL,
  `file_path` varchar(255) DEFAULT NULL,
  `file_name` varchar(255) DEFAULT NULL,
  `upload_date` date DEFAULT NULL,
  `upload_time` time DEFAULT NULL,
  PRIMARY KEY (`file_id`,`file_model_id`),
  KEY `file_model_id_idx` (`file_model_id`),
  CONSTRAINT `file_model_id` FOREIGN KEY (`file_model_id`) REFERENCES `models` (`model_id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `model_files`
--

LOCK TABLES `model_files` WRITE;
/*!40000 ALTER TABLE `model_files` DISABLE KEYS */;
INSERT INTO `model_files` VALUES (4,8,'42f6371b18c8f3fb7b3b2360ed96799b0c3184aca80e6b8788.ipynb','Model_train','2024-05-05','22:24:15'),(5,8,'ece4ae88152f1c58b96c1e8bd5a617a65873820ff3a99f4610.csv','Dataset_mt_car','2024-05-05','22:24:42'),(10,19,'340eae922037a8ac767bd846eaf9a42f2256c461c4d1e91faa.csv','dataset','2024-05-10','01:33:04');
/*!40000 ALTER TABLE `model_files` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `models`
--

DROP TABLE IF EXISTS `models`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `models` (
  `model_id` int NOT NULL AUTO_INCREMENT,
  `user_id` int NOT NULL,
  `model_name` varchar(255) NOT NULL,
  `model_file_type` varchar(15) NOT NULL,
  PRIMARY KEY (`model_id`,`user_id`),
  KEY `user_id_idx` (`user_id`),
  CONSTRAINT `user_id` FOREIGN KEY (`user_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=24 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `models`
--

LOCK TABLES `models` WRITE;
/*!40000 ALTER TABLE `models` DISABLE KEYS */;
INSERT INTO `models` VALUES (3,13,'Iris Models','pkl'),(4,13,'Simple neural network model','onnx'),(7,13,'SVM Breast Cancer Model ','joblib'),(8,13,'Car Model','joblib'),(15,13,'Iris Test Model','pkl'),(19,13,'Joblib visualize test','joblib'),(20,13,'Random Forest For Iris Model','joblib'),(23,13,'densenet-timm121','onnx');
/*!40000 ALTER TABLE `models` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `post_comments`
--

DROP TABLE IF EXISTS `post_comments`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `post_comments` (
  `comment_id` int NOT NULL AUTO_INCREMENT,
  `com_post_id` int NOT NULL,
  `com_user_id` int NOT NULL,
  `comment` text CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,
  `com_date` date NOT NULL,
  `com_time` time NOT NULL,
  PRIMARY KEY (`comment_id`),
  KEY `com_post_id_idx` (`com_post_id`),
  KEY `com_user_id_idx` (`com_user_id`),
  CONSTRAINT `com_post_id` FOREIGN KEY (`com_post_id`) REFERENCES `posts` (`post_id`) ON DELETE CASCADE,
  CONSTRAINT `com_user_id` FOREIGN KEY (`com_user_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `post_comments`
--

LOCK TABLES `post_comments` WRITE;
/*!40000 ALTER TABLE `post_comments` DISABLE KEYS */;
INSERT INTO `post_comments` VALUES (25,16,14,'<p><strong>It works !!</strong></p>\r\n','2024-05-15','23:30:27');
/*!40000 ALTER TABLE `post_comments` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `post_data`
--

DROP TABLE IF EXISTS `post_data`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `post_data` (
  `post_id` int NOT NULL,
  `post_title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,
  `post_body` text CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,
  `post_date` date NOT NULL,
  `post_time` time NOT NULL,
  PRIMARY KEY (`post_id`),
  CONSTRAINT `post_id` FOREIGN KEY (`post_id`) REFERENCES `posts` (`post_id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `post_data`
--

LOCK TABLES `post_data` WRITE;
/*!40000 ALTER TABLE `post_data` DISABLE KEYS */;
INSERT INTO `post_data` VALUES (9,'Array.reduce() is Goated ?','<p>The title says it all ?. I want to talk about my all-time favorite javascript array method:&nbsp;<strong>Array.reduce()</strong>. I know there are a lot of contenders out there, but hear me out. reduce() is not just a method; it&#39;s a way of life✨.</p>\r\n\r\n<p>I&#39;m not going to lie though, when I first started and discovered reduce, it was a little intimidating. It took me a while before I was confidently using it everywhere in my code. But when I did, it was a game changer. Suddenly, I could perform complex operations on arrays with ease, transforming them into whatever I needed. My code become faster and cleaner.</p>\r\n\r\n<p>But don&#39;t just take my word for it. Let me show you some of the things you can achieve with reduce(). It&#39;s time to dive into Array.reduce() and discover why it&#39;s absolutely goated! ?</p>\r\n\r\n<h2>9 Use Cases for Array.reduce() ?</h2>\r\n\r\n<h3>Use Case 1: Summing Numbers</h3>\r\n\r\n<p>One of the most straightforward use cases for reduce() is summing up a bunch of numbers. Let&#39;s say you have an array of integers and you want to find the total sum.</p>\r\n\r\n<pre>\r\nconst numbers: number[] = [1, 2, 3, 4, 5];\r\nconst sum: number = numbers.reduce((acc, curr) =&gt; acc + curr, 0);\r\nconsole.log(sum); // Output: 15\r\n</pre>\r\n\r\n<p>Boom! With just one line of code, you&#39;ve calculated the sum of all elements in the array. The initial value of the accumulator is set to 0, and in each iteration, we add the current element to the accumulator.</p>\r\n\r\n<p>** Bonus: If you choose to leave out the starting value, reduce will just use the first item in the array. However I tend to always include an initial value, so it&#39;s easier to read.</p>\r\n\r\n<h3>Use Case 2: Flattening Arrays</h3>\r\n\r\n<p>Have you ever found yourself with an array of arrays and thought, &quot;I wish I could flatten this into a single array&quot;?</p>\r\n\r\n<pre>\r\n<code>const nestedArray: number[][] = [[1, 2], [3, 4], [5, 6]];\r\nconst flattenedArray: number[] = nestedArray.reduce((acc, curr) =&gt; acc.concat(curr), []);\r\nconsole.log(flattenedArray); // Output: [1, 2, 3, 4, 5, 6]\r\n</code></pre>\r\n\r\n<p>In this example, we start with an empty array as the initial accumulator value. Then, in each iteration, we concatenate the current sub-array to the accumulator using the concat() method. By the end, we have a perfect flattened array.</p>\r\n\r\n<p>I know that you can also do this with&nbsp;<code>Array.flat()</code>. However, it&#39;s important to know how to use reduce, in case you want to perform extra operations on each item.</p>\r\n\r\n<h3>Use Case 3: Grouping Objects</h3>\r\n\r\n<p>Imagine you have an array of objects, and you want to group them based on a specific property. reduce() is the perfect tool for the job.</p>\r\n\r\n<pre>\r\n<code>interface Person {\r\n  name: string;\r\n  age: number;\r\n}\r\n\r\nconst people: Person[] = [\r\n  { name: &#39;Alice&#39;, age: 25 },\r\n  { name: &#39;Bob&#39;, age: 30 },\r\n  { name: &#39;Charlie&#39;, age: 25 },\r\n  { name: &#39;Dave&#39;, age: 30 }\r\n];\r\n\r\nconst groupedByAge: { [key: number]: Person[] } = people.reduce((acc, curr) =&gt; {\r\n  if (!acc[curr.age]) {\r\n    acc[curr.age] = [];\r\n  }\r\n  acc[curr.age].push(curr);\r\n  return acc;\r\n}, {});\r\n\r\nconsole.log(groupedByAge);\r\n/*\r\nOutput:\r\n{\r\n  &#39;25&#39;: [{ name: &#39;Alice&#39;, age: 25 }, { name: &#39;Charlie&#39;, age: 25 }],\r\n  &#39;30&#39;: [{ name: &#39;Bob&#39;, age: 30 }, { name: &#39;Dave&#39;, age: 30 }]\r\n}\r\n*/\r\n</code></pre>\r\n\r\n<p>In this case, we use an object as the initial accumulator value. We check if the accumulator already has a property for the current age. If not, we create an empty array for that age. Then, we push the current object into the corresponding age array. By the end, we have an object where the keys are the ages, and the values are arrays of people with that age.</p>\r\n\r\n<p>You can now also now the newer&nbsp;<code>groupBy</code>&nbsp;method, however, this tried and true classic is important to understand.</p>\r\n\r\n<h3>Use Case 4: Creating Lookup Maps</h3>\r\n\r\n<p>My personal favorite is using reduce() to create lookup maps from arrays. It&#39;s a game-changer when it comes to performance and code readability. Stop using those slow find() or filter() calls.</p>\r\n\r\n<pre>\r\n<code>interface Product {\r\n  id: number;\r\n  name: string;\r\n  price: number;\r\n}\r\n\r\nconst products: Product[] = [\r\n  { id: 1, name: &#39;Laptop&#39;, price: 999 },\r\n  { id: 2, name: &#39;Phone&#39;, price: 699 },\r\n  { id: 3, name: &#39;Tablet&#39;, price: 499 },\r\n];\r\n\r\nconst productMap: { [key: number]: Product } = products.reduce((acc, curr) =&gt; {\r\n  acc[curr.id] = curr;\r\n  return acc;\r\n}, {});\r\n\r\nconsole.log(productMap);\r\n/*\r\nOutput:\r\n{\r\n  &#39;1&#39;: { id: 1, name: &#39;Laptop&#39;, price: 999 },\r\n  &#39;2&#39;: { id: 2, name: &#39;Phone&#39;, price: 699 },\r\n  &#39;3&#39;: { id: 3, name: &#39;Tablet&#39;, price: 499 }\r\n}\r\n*/\r\n\r\n// Accessing a product by ID\r\nconst laptop: Product = productMap[1];\r\nconsole.log(laptop); // Output: { id: 1, name: &#39;Laptop&#39;, price: 999 }\r\n</code></pre>\r\n\r\n<p>By using reduce() to create a lookup map, you can access elements by their unique identifier in constant time complexity. No more looping through the array to find a specific item.</p>\r\n\r\n<h3>Use Case 5: Counting Occurrences</h3>\r\n\r\n<p>Ever needed to count the occurrences of elements in an array? reduce() has got you covered.</p>\r\n\r\n<pre>\r\n<code>const fruits: string[] = [&#39;apple&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;orange&#39;, &#39;banana&#39;, &#39;apple&#39;];\r\n\r\nconst fruitCounts: { [key: string]: number } = fruits.reduce((acc, curr) =&gt; {\r\n  acc[curr] = (acc[curr] || 0) + 1;\r\n  return acc;\r\n}, {});\r\n\r\nconsole.log(fruitCounts);\r\n/*\r\nOutput:\r\n{\r\n  &#39;apple&#39;: 3,\r\n  &#39;banana&#39;: 2,\r\n  &#39;orange&#39;: 1\r\n}\r\n*/\r\n</code></pre>\r\n\r\n<p>In this example, we initialize an empty object as the accumulator. For each fruit in the array, we check if it already exists as a property in the accumulator object. If it does, we increment its count by 1; otherwise, we initialize it to 1. The result is an object that tells us how many times each fruit appears in the array.</p>\r\n\r\n<h3>Use Case 6: Composing Functions</h3>\r\n\r\n<p>Functional programming enthusiasts will enjoy this one. reduce() is a powerful tool for composing functions. You can use it to create a pipeline of functions that transform data step by step.</p>\r\n\r\n<pre>\r\n<code>const add5 = (x: number): number =&gt; x + 5;\r\nconst multiply3 = (x: number): number =&gt; x * 3;\r\nconst subtract2 = (x: number): number =&gt; x - 2;\r\n\r\nconst composedFunctions: ((x: number) =&gt; number)[] = [add5, multiply3, subtract2];\r\n\r\nconst result: number = composedFunctions.reduce((acc, curr) =&gt; curr(acc), 10);\r\nconsole.log(result); // Output: 43\r\n</code></pre>\r\n\r\n<p>In this example, we have an array of functions that we want to apply in sequence to an initial value of 10. We use reduce() to iterate over the functions, passing the result of each function as the input to the next one. The final result is the outcome of applying all the functions in the composed order.</p>\r\n\r\n<h3>Use Case 7: Implementing a Simple Redux-like State Management</h3>\r\n\r\n<p>If you&#39;ve worked with Redux, you know how powerful it is for managing state in applications. Guess what? You can use reduce() to implement a simple Redux-like state management system.</p>\r\n\r\n<pre>\r\n<code>interface State {\r\n  count: number;\r\n  todos: string[];\r\n}\r\n\r\ninterface Action {\r\n  type: string;\r\n  payload?: any;\r\n}\r\n\r\nconst initialState: State = {\r\n  count: 0,\r\n  todos: [],\r\n};\r\n\r\nconst actions: Action[] = [\r\n  { type: &#39;INCREMENT_COUNT&#39; },\r\n  { type: &#39;ADD_TODO&#39;, payload: &#39;Learn Array.reduce()&#39; },\r\n  { type: &#39;INCREMENT_COUNT&#39; },\r\n  { type: &#39;ADD_TODO&#39;, payload: &#39;Master TypeScript&#39; },\r\n];\r\n\r\nconst reducer = (state: State, action: Action): State =&gt; {\r\n  switch (action.type) {\r\n    case &#39;INCREMENT_COUNT&#39;:\r\n      return { ...state, count: state.count + 1 };\r\n    case &#39;ADD_TODO&#39;:\r\n      return { ...state, todos: [...state.todos, action.payload] };\r\n    default:\r\n      return state;\r\n  }\r\n};\r\n\r\nconst finalState: State = actions.reduce(reducer, initialState);\r\nconsole.log(finalState);\r\n/*\r\nOutput:\r\n{\r\n  count: 2,\r\n  todos: [&#39;Learn Array.reduce()&#39;, &#39;Master TypeScript&#39;]\r\n}\r\n*/\r\n</code></pre>\r\n\r\n<p>In this example, we have an initial state object and an array of actions. We define a reducer function that takes the current state and an action, and returns a new state based on the action type. By using reduce(), we can apply each action to the state in sequence, resulting in the final state. It&#39;s like having a mini-Redux.</p>\r\n\r\n<h3>Use Case 8: Generating Unique Values</h3>\r\n\r\n<p>Sometimes, you might have an array with duplicate values, and you need to extract only the unique ones. reduce() can help you accomplish that with ease.</p>\r\n\r\n<pre>\r\n<code>const numbers: number[] = [1, 2, 3, 2, 4, 3, 5, 1, 6];\r\n\r\nconst uniqueNumbers: number[] = numbers.reduce((acc, curr) =&gt; {\r\n  if (!acc.includes(curr)) {\r\n    acc.push(curr);\r\n  }\r\n  return acc;\r\n}, []);\r\n\r\nconsole.log(uniqueNumbers); // Output: [1, 2, 3, 4, 5, 6]\r\n</code></pre>\r\n\r\n<p>Here, we initialize an empty array as the accumulator. For each number in the original array, we check if it already exists in the accumulator using the includes() method. If it doesn&#39;t, we push it into the accumulator array. The final result is an array containing only the unique values from the original array.</p>\r\n\r\n<h3>Use Case 9: Calculating Average</h3>\r\n\r\n<p>Want to calculate the average of a set of numbers? reduce() has got your back!</p>\r\n\r\n<pre>\r\n<code>const grades: number[] = [85, 90, 92, 88, 95];\r\n\r\nconst average: number = grades.reduce((acc, curr, index, array) =&gt; {\r\n  acc += curr;\r\n  if (index === array.length - 1) {\r\n    return acc / array.length;\r\n  }\r\n  return acc;\r\n}, 0);\r\n\r\nconsole.log(average); // Output: 90\r\n</code></pre>\r\n\r\n<p>In this example, we initialize the accumulator to 0. We iterate over each grade and add it to the accumulator. When we reach the last element (checked using the index and array.length), we divide the accumulator by the total number of grades to calculate the average.</p>\r\n\r\n<h2>Performance Considerations ?️</h2>\r\n\r\n<p>While Array.reduce() is incredibly powerful and versatile, it&#39;s important to be aware of potential performance drawbacks, especially when dealing with large arrays or complex operations. One common pitfall is creating new objects or arrays in each iteration of reduce(), which can lead to excessive memory allocation and impact performance.</p>\r\n\r\n<p>For example, consider the following code:</p>\r\n\r\n<pre>\r\n<code>const numbers: number[] = [1, 2, 3, 4, 5];\r\n\r\nconst doubledNumbers: number[] = numbers.reduce((acc, curr) =&gt; {\r\n  return [...acc, curr * 2];\r\n}, []);\r\n\r\nconsole.log(doubledNumbers); // Output: [2, 4, 6, 8, 10]\r\n</code></pre>\r\n\r\n<p>In this case, we&#39;re using the spread operator (...) to create a new array in each iteration, which can be inefficient. Instead, we can optimize the code by mutating the accumulator array directly:</p>\r\n\r\n<pre>\r\n<code>const numbers: number[] = [1, 2, 3, 4, 5];\r\n\r\nconst doubledNumbers: number[] = numbers.reduce((acc, curr) =&gt; {\r\n  acc.push(curr * 2);\r\n  return acc;\r\n}, []);\r\n\r\nconsole.log(doubledNumbers); // Output: [2, 4, 6, 8, 10]\r\n</code></pre>\r\n\r\n<p>By mutating the accumulator array using push(), we avoid creating new arrays in each iteration, resulting in better performance.</p>\r\n\r\n<p>Similarly, when working with objects, it&#39;s more efficient to mutate the accumulator object directly rather than creating a new object with the spread operator:</p>\r\n\r\n<pre>\r\n<code>const people: Person[] = [\r\n  { name: &#39;Alice&#39;, age: 25 },\r\n  { name: &#39;Bob&#39;, age: 30 },\r\n  { name: &#39;Charlie&#39;, age: 25 },\r\n  { name: &#39;Dave&#39;, age: 30 }\r\n];\r\n\r\nconst groupedByAge: { [key: number]: Person[] } = people.reduce((acc, curr) =&gt; {\r\n  if (!acc[curr.age]) {\r\n    acc[curr.age] = [];\r\n  }\r\n  acc[curr.age].push(curr);\r\n  return acc;\r\n}, {});\r\n</code></pre>\r\n\r\n<p>By mutating the accumulator object directly, we optimize the performance of the reduce() operation.</p>\r\n\r\n<p>However, it&#39;s worth noting that in some cases, creating new objects or arrays in each iteration may be necessary or more readable. It&#39;s important to strike a balance between performance and code clarity based on your specific use case and the size of the data you&#39;re working with.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>There you have it. Nine incredible use cases that showcase the power and versatility of Array.reduce(). From summing numbers to flattening arrays, grouping objects to creating lookup maps, counting occurrences to composing functions, and even implementing state management and calculating averages,&nbsp;<code>Array.reduce()</code>&nbsp;proves to be a powerful tool in your js toolkit.</p>\r\n\r\n<p>What do you think? What is your favorite array method and why?<br />\r\nThanks for reading, and may the power of reduce() be with you. ✨?✨</p>\r\n','2024-05-14','17:33:27'),(10,'Machine Learning With Python','<h2><img alt=\"Machine Learning\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--5Z8hUOE---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media.istockphoto.com/vectors/machine-learning-3-step-infographic-artificial-intelligence-machine-vector-id962219860%3Fk%3D6%26m%3D962219860%26s%3D612x612%26w%3D0%26h%3DyricYyUqZbILMHp3IvtenS3xbRDhu1w1u5kk2az5tbo%3D\" style=\"height:422px; margin:2px; width:612px\" /></h2>\r\n\r\n<h2>&nbsp;</h2>\r\n\r\n<h2>Small scale machine learning projects to understand the core concepts (order: oldest to newest)</h2>\r\n\r\n<ul>\r\n	<li>Topic Modelling using&nbsp;<strong>Latent Dirichlet Allocation</strong>&nbsp;with newsgroups20 dataset, implemented with Python and Scikit-Learn</li>\r\n	<li>Implemented a simple&nbsp;<strong>neural network</strong>&nbsp;built with Keras on MNIST dataset</li>\r\n	<li>Stock Price Forecasting on Google using&nbsp;<strong>Linear Regression</strong></li>\r\n	<li>Implemented a simple a&nbsp;<strong>social network</strong>&nbsp;to learn basics of Python</li>\r\n	<li>Implemented&nbsp;<strong>Naives Bayes Classifier</strong>&nbsp;to filter spam messages on SpamAssasin Public Corpus</li>\r\n	<li><strong>Churn Prediction Model</strong>&nbsp;for banking dataset using Keras and Scikit-Learn</li>\r\n	<li>Implemented&nbsp;<strong>Random Forest</strong>&nbsp;from scratch and built a classifier on Sonar dataset from UCI repository</li>\r\n	<li>Simple Linear Regression in Python on sample dataset</li>\r\n	<li><strong>Multiple Regression</strong>&nbsp;in Python on sample dataset</li>\r\n	<li><strong>PCA and scaling</strong>&nbsp;sample stock data in Python [working_with_data]</li>\r\n	<li><strong>Decision Trees</strong>&nbsp;in Python on sample dataset</li>\r\n	<li><strong>Logistic Regression</strong>&nbsp;in Python on sample dataset</li>\r\n	<li>Built a neural network in Python to defeat a captcha system</li>\r\n	<li>Helper methods include commom operations used in&nbsp;<strong>Statistics, Probability, Linear Algebra and Data Analysis</strong></li>\r\n	<li><strong>K-means clustering</strong>&nbsp;with example data;&nbsp;<strong>clustering colors</strong>&nbsp;with k-means;&nbsp;<strong>Bottom-up Hierarchical Clustering</strong></li>\r\n	<li>Generating Word Clouds</li>\r\n	<li>Sentence generation using n-grams</li>\r\n	<li>Sentence generation using&nbsp;<strong>Grammars and Automata Theory; Gibbs Sampling</strong></li>\r\n	<li>Topic Modelling using Latent Dirichlet Analysis (LDA)</li>\r\n	<li>Wrapper for using Scikit-Learn&#39;s&nbsp;<strong>GridSearchCV</strong>&nbsp;for a&nbsp;<strong>Keras Neural Network</strong></li>\r\n</ul>\r\n','2024-05-14','17:35:45'),(11,'Machine Learning and Artificial Intelligence','<h2>Machines are becoming more intelligent as a result of big data.</h2>\r\n\r\n<p>Machine gaining knowledge of (ML) and synthetic intelligence (AI) have become dominant trouble-fixing strategies in lots of regions of studies and industry, now no longer least due to the latest successes of deep gaining knowledge of (DL). However, the equation AI=ML=DL, as these days recommended withinside the news, blogs, and media, falls too short. These fields proportion the equal essential hypotheses: computation is a beneficial manner to version wise conduct in machines. What sort of computation and the way to software it? This isn&#39;t always the proper query. Computation neither guidelines out seek, logical, and probabilistic strategies, nor (deep) (un)supervised and reinforcement gaining knowledge of strategies, amongst others, as computational fashions do encompass all of them. They supplement every different, and the following step forward lies now no longer handiest in pushing every of them however additionally in combining them.</p>\r\n\r\n<p>Big Data isn&#39;t any fad. The international is developing at an exponential charge and so is the scale of the records gathered throughout the globe. Data is turning into greater significant and contextually applicable, breaking new grounds for system gaining knowledge of (ML), specially for deep gaining knowledge of (DL) and synthetic intelligence (AI), transferring them out of studies labs into production (Jordan and Mitchell, 2015). The trouble has shifted from accumulating large quantities of records to information it&mdash;turning it into expertise, conclusions, and movements. Multiple studies disciplines, from cognitive sciences to biology, finance, physics, and social sciences, in addition to many agencies trust that records-pushed and &ldquo;wise&rdquo; answers are vital to clear up lots of their key problems. High-throughput genomic and proteomic experiments may be used to allow personalised medicine. Large records units of seek queries may be used to enhance statistics retrieval. Historical weather records may be used to apprehend worldwide warming and to higher are expecting weather. Large quantities of sensor readings and hyperspectral photographs of flora may be used to discover drought situations and to benefit insights into whilst and the way pressure influences plant increase and improvement and in flip the way to counterattack the trouble of globalwide hunger. Game records can flip pixels into movements inside video video games, at the same time as observational records can assist allow robots to apprehend complicated and unstructured environments and to research manipulation talents.</p>\r\n\r\n<p>However, is AI, ML, and DL simply synonymous, as these days recommended withinside the news, blogs, and media? For example, whilst AlphaGo (Silver et al., 2016) defeated South Korean Master Lee Se-dol withinside the board sport Go in 2016, the phrases AI, ML, and DL have been utilized by the media to explain how AlphaGo won. In addition to this, even Gartner&#39;s list (Panetta, 2017) of pinnacle 10 Strategic Trends for 2018 places (narrow) AI on the very pinnacle, specifying it as &ldquo;along with fairly scoped system-gaining knowledge of answers that concentrate on a selected project.&rdquo;</p>\r\n\r\n<h3>Machine Learning and Artificial Intelligence</h3>\r\n\r\n<p>Artificial intelligence and ML are very a great deal related. According to McCarthy (2007), one of the founders of the field,</p>\r\n\r\n<p>AI is &ldquo;the technological know-how and engineering of creating wise machines, specially wise laptop applications. It is associated with the same project of the use of computer systems to apprehend human intelligence, however AI does now no longer ought to confine itself to strategies which are biologically observable.&rdquo;</p>\r\n\r\n<p>This within reason widespread and consists of a couple of obligations which includes abstractly reasoning and generalizing approximately the international, fixing puzzles, making plans the way to reap goals, transferring round withinside the international, spotting gadgets and sounds, speaking, translating, acting social or enterprise transactions, innovative paintings (e.g., developing artwork or poetry), and controlling robots. Moreover, the conduct of a system isn&#39;t always simply the final results of the software, it&#39;s also laid low with its &ldquo;body&rdquo; and the enviroment it&#39;s miles bodily embedded in. To maintain it simple, however, if you may write a totally smart software that has, say, human-like conduct, it is able to be AI. But except it mechanically learns from records, it isn&#39;t always ML:</p>\r\n\r\n<p>ML is the technological know-how this is &ldquo;involved with the query of the way to assemble laptop applications that mechanically enhance with revel in,&rdquo; (Mitchell, 1997).</p>\r\n\r\n<p>So, AI and ML are each approximately building wise laptop applications, and DL, being an example of ML, isn&#39;t anyt any exception. Deep gaining knowledge of (LeCun et al., 2015; Goodfellow et al., 2016), which has executed great profits in lots of domain names spanning from item recognition, speech recognition, and control, may be regarded as building laptop applications, specifically programming layers of abstraction in a differentiable manner the use of reusable systems which includes convolution, pooling, vehiclemobile encoders, variational inference networks, and so on. In different words, we update the complexity of writing algorithms, that cowl each eventuality, with the complexity of locating the proper trendy define of the algorithms&mdash;withinside the shape of, for example, a deep neural network&mdash;and processing records. By distinctive feature of the generality of neural networks&mdash;they&#39;re trendy feature approximators&mdash;schooling them is records hungry and normally calls for big classified schooling units. While benchmark schooling units for item recognition, keep masses or lots of examples in line with elegance label, for lots AI applications, developing classified schooling records is the maximum time-eating and highly-priced a part of DL. Learning to play video video games may also require masses of hours of schooling revel in and/or very highly-priced computing power. In contrast, writing an AI set of rules that covers each eventuality of a project to clear up, say, reasoning approximately records and expertise to label records mechanically (Ratner et al., 2016; Roth, 2017) and, in flip, make, for example, DL much less records-hungry&ndash;is lots of guide paintings, however we realize what the set of rules does with the aid of using layout and that it is able to take a look at and that it is able to greater without problems apprehend the complexity of the trouble it solves. When a system has to have interaction with a human, this appears to be specially valuable.</p>\r\n\r\n<p>This illustrates that ML and AI are certainly comparable, however now no longer pretty the equal. Artificial intelligence is set trouble fixing, reasoning, and gaining knowledge of in trendy. Machine gaining knowledge of is specially approximately gaining knowledge of&mdash;gaining knowledge of from examples, from definitions, from being told, and from conduct. The simplest manner to consider their dating is to visualise them as concentric circles with AI first and ML sitting internal (with DL becoming internal each), for the reason that ML additionally calls for writing algorithms that cowl each eventuality, specifically, of the gaining knowledge of manner. The essential factor is they proportion the concept of the use of computation because the language for wise conduct. What sort of computation is used and the way have to it&#39;s programed? This isn&#39;t always the proper query. Computation neither guidelines out seek, logical, probabilistic, and constraint programming strategies nor (deep) (un)supervised and reinforcement gaining knowledge of strategies, amongst others, however does, as a computational version, incorporate all of those strategies.</p>\r\n\r\n<p>Reconsidering AlphaGo: AlphaGo and its successor AlphaGo Zero (Silver et al., 2017) each integrate DL and tree seek&mdash;ML and AI. Alternatively, the &ldquo;Allen AI Science Challenge&rdquo; (Schoenick et al., 2017) have to be considered. The project changed into to recognize a paragraph that states a technological know-how trouble, on the center faculty stage after which to reply a a couple of-desire query. All prevailing fashions hired ML but did not byskip the take a look at at the extent of a equipped center schooler. All winners argued that it changed into clean that making use of a deeper, semantic stage of reasoning with clinical expertise to the query and answers, is the important thing to attaining genuine intelligence. In different words, AI has to cowl expertise, reasoning, and gaining knowledge of, the use of programmed and gaining knowledge of-primarily based totally programmed fashions in a blended fashion.</p>\r\n\r\n<h3>The Joint Effort to Define Intelligent Machine Behavior</h3>\r\n\r\n<p>Using computation because the not unusualplace language, we&#39;ve got come an extended manner, however the adventure in advance continues to be long. None of ultra-modern wise machines come near the breadth and intensity of human intelligence. In many real-international applications, as illustrated with the aid of using AlphaGo and the Allen AI Science Challenge, it&#39;s miles uncertain whether or not trouble components falls smartly into completely gaining knowledge of. The trouble may also nicely have a big component, which may be exceptional modeled the use of an AI set of rules with out the gaining knowledge of component, however there can be extra constraints or lacking expertise that take the trouble outdoor its regime, and gaining knowledge of may also assist to fill the gap. Similarly, programmed expertise and reasoning may also assist novices to fill their gaps. There is a symmetric distinction among AI and ML, and wise conduct in machines is a joint quest, with many substantial and captivating open studies problems:</p>\r\n\r\n<p>&bull; How can computer systems motive approximately and research with complicated records which includes multimodal records, graphs, and unsure databases?</p>\r\n\r\n<p>&bull; How can preexisting expertise be exploited?</p>\r\n\r\n<p>&bull; How are we able to make sure that gaining knowledge of machines satisfy given constraints and offer sure guarantees?</p>\r\n\r\n<p>&bull; How can computer systems autonomously determine the exceptional illustration for the records at hand?</p>\r\n\r\n<p>&bull; How will we orchestrate distinctive algorithms, related to discovered or now no longer discovered ones?</p>\r\n\r\n<p>&bull; How will we democratize ML and AI?</p>\r\n\r\n<p>&bull; Can discovered effects be bodily manageable or without problems understood with the aid of using us?</p>\r\n\r\n<p>&bull; How will we make computer systems research with us withinside the loop?</p>\r\n\r\n<p>&bull; How will we make computer systems research with much less assist and records supplied with the aid of using us?</p>\r\n\r\n<p>&bull; Can they autonomously determine the exceptional constraints and algorithms for a project at hand?</p>\r\n\r\n<p>&bull; How will we make computer systems research as a great deal approximately the international, in a rapid, flexible, and explainable manner, as people?</p>\r\n\r\n<p>Answering those and different comparable questions will placed the dream of wise and accountable machines into reach. Fully programmed computations, collectively with gaining knowledge of-primarily based totally programmed computations, will assist to higher generalize, past the precise records that we&#39;ve got visible, whether or not a brand new pronunciation of a phrase or an photograph will appreciably vary from the ones we&#39;ve got visible before. They permit us to move appreciably past supervised gaining knowledge of, closer to incidential and unsupervised gaining knowledge of, which does now no longer rely a lot on classified schooling records. They offer a not unusualplace floor for continuous, deep, and symbolic manipulations. They permit us to derive insights from cognitive technological know-how and different disciplines for ML and AI. They permit us to consciousness greater on obtaining not unusualplace feel expertise and clinical reasoning, at the same time as additionally presenting a clean course for democratizing ML-AI technology, as recommended with the aid of using De Raedt et al. (2016) and Kordjamshidi et al. (2018). Building wise structures calls for understanding in laptop technological know-how and giant programming talents to paintings with numerous system reasoning and gaining knowledge of strategies at a instead low-stage of abstraction. Building wise structures additionally calls for giant trial and blunders exploration for version selection, records cleaning, characteristic selection, and parameter tuning. There is honestly a loss of theoretical information that might be used to do away with those subtleties. Conventional programming languages and software program engineering paradigms have additionally now no longer been designed to deal with the demanding situations confronted with the aid of using AI and ML practitioners, which includes coping with messy, real-international records on the proper stage of abstraction and with continuously converting trouble definitions. Finally, records-pushed technological know-how is an exploratory project. Starting from a tremendous basis of area professional expertise, applicable standards in addition to heuristic fashions can change, or even the trouble definition is in all likelihood to be reshaped simultaneously in mild of latest evidence. Interactive ML and AI can shape the idea for brand spanking new strategies that version dynamically evolving objectives and comprise professional expertise at the fly. To permit the area professional to persuade records-pushed studies, the prediction manner moreover wishes to be sufficiently transparent.</p>\r\n\r\n<h2>Conclusions</h2>\r\n\r\n<p>Machine gaining knowledge of and AI supplement every different, and the following step forward lies now no longer handiest in pushing every of them however additionally in combining them. Our algorithms have to support (re)trainable, (re)composable fashions of computation and facilitate reasoning and interplay with admire to those fashions on the proper stage of abstraction. Multiple disciplines and studies regions want to collaborate to force those breakthroughs. Using computation because the not unusualplace language has the capacity for progressing gaining knowledge of standards and inferring statistics this is each smooth and hard for people to acquire.</p>\r\n\r\n<p>To this end, the &ldquo;Machine Learning and Artificial intelligence&rdquo; phase in Frontiers in Big Data welcomes foundational and carried out papers in addition to replication research from a huge variety of subjects underpinning ML, AI, and their interplay. It will foster the scholarly dialogue of the reasons and results of achievements presenting a right attitude at the acquired effects. Using the not unusualplace language of computation, we will completely apprehend the way to reap wise conduct in machines.</p>\r\n','2024-05-14','17:36:57'),(12,'Machine Learning From Scratch: Linear Regression step by step','<h2>Introduction</h2>\r\n\r\n<p>If you want to get into machine learning, I can&rsquo;t think of a better way than start by learning Linear regression.<br />\r\nIn this article, you will get a clear vision on what is linear regression, hypothesis function, and cost function.<br />\r\nLinear Regression is one of the most straightforward algorithms.<br />\r\nIt is easy to understand and known by all data scientists and machine learning engineers.<br />\r\nalso, it&rsquo;s a very good starting point to learn other machine learning algorithms.<br />\r\nyet, linear regression is a very useful and efficient algorithm,<br />\r\nwhen used in the right problem with the right dataset.<br />\r\nIn this article, we will go step by step to understand how linear regression is working.<br />\r\nSo let&rsquo;s not waste more time and get started.</p>\r\n\r\n<h2>What is a regression in the first place ?!</h2>\r\n\r\n<p>In statistical modeling, regression is the process of forecasting a dependent variable (Y). based on one or more of an independent variable (X).<br />\r\nin other words, a regression model is a statistical tool used to find the relation between the output (Y). and the input (X).</p>\r\n\r\n<h1>Linear Regression Algorithm.</h1>\r\n\r\n<p>The linear regression model is the basic form of regression analysis,<br />\r\nit estimates that the relation between the dependent and independent variables is linear.<br />\r\nWe know from linear algebra that the straight-line equation :<br />\r\ny = mx+ b</p>\r\n\r\n<p>we will use this equation to predict the output variable (y) with the relation to&nbsp;<strong>x( feature), m ( the slope of a straight line ), and b ( intersection of st-line with y-axis ).</strong></p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--wLPEqkIo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/moe1r4t91xdkzelboh2o.png\"><img alt=\"straight line graph\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--wLPEqkIo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/moe1r4t91xdkzelboh2o.png\" /></a></p>\r\n\r\n<p>equation of blue line&nbsp;<strong>y = x + 2</strong><br />\r\nequation of red line&nbsp;<strong>y = 2x + 0</strong></p>\r\n\r\n<h2>hypothesis equation</h2>\r\n\r\n<p>hypothesis equation is the st-line equation used in the machine learning algorithm with optimized parameters to best fit the data.</p>\r\n\r\n<p>it&rsquo;s a model that maps inputs to outputs and is used to make predictions.</p>\r\n\r\n<p>the basic idea here or in any machine learning problem is:<br />\r\nfeed the model with data.</p>\r\n\r\n<p>test the model by predicting values you already know.<br />\r\noptimize the model for better predictions, and now you have a working machine learning model that can predict outputs.<br />\r\nlet&rsquo;s change the parameters of the st-line equation&nbsp;<strong>m &rarr; &theta;2, b &rarr; &theta;1</strong></p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--Zo5uLUOD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h8jbqgdvgdb7joche08s.jpg\"><img alt=\"hypothesis function\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--Zo5uLUOD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h8jbqgdvgdb7joche08s.jpg\" /></a></p>\r\n\r\n<p>Let&#39;s give different values for theta (&theta;1,&theta;2) and see how the line changes.</p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--t61JX1nd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/s2555u769mjslymerqq7.png\"><img alt=\"different values for theta 1 and 2 \" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--t61JX1nd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/s2555u769mjslymerqq7.png\" /></a></p>\r\n\r\n<p>we see that as &theta;2 changes the angle (slope) of the st-line changes and as &theta;1 changes the line intersection with the y-axis changes.</p>\r\n\r\n<h2>Accuracy</h2>\r\n\r\n<p>when our models make predictions, it&rsquo;s not 100% accurate<br />\r\nin classification problems where the prediction is binary ( 0 or 1 )</p>\r\n\r\n<p>the ratio between right predictions and wrong predictions is the model accuracy</p>\r\n\r\n<p>if our model predicts 7 out of 10 right predictions so, the accuracy of the model equal to 70%</p>\r\n\r\n<h2>Cost function</h2>\r\n\r\n<p>Now we have a model that makes predictions by the hypothesis equation and we want to know how well are those predictions<br />\r\nin simple words, the cost function tests how well the model performs,<br />\r\nif the cost function is equal to zero, then the model is 100% accurate.</p>\r\n\r\n<p>different from accuracy cost function is not a percentage<br />\r\nbut, calculated by the error difference between the predicted value (ŷ) and expected value ( True value) (y).</p>\r\n\r\n<h2>Mean Squared Error</h2>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--F0p1CUTV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b9hm0rf60gsq3sctjr0f.png\"><img alt=\"mean squared error formula\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--F0p1CUTV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b9hm0rf60gsq3sctjr0f.png\" /></a></p>\r\n\r\n<p>MSE = Mean Squared Error<br />\r\nm = number of training examples ( samples ).<br />\r\ni = index of sample<br />\r\nŷ = predicted value<br />\r\ny = expected value</p>\r\n\r\n<p>the predicted value ŷ is the output of the hypothesis while the expected value y is the value of training examples.</p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--Z15xrSXA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6o2yc2fehitx38ama2q5.png\"><img alt=\"regression example.\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--Z15xrSXA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6o2yc2fehitx38ama2q5.png\" /></a></p>\r\n\r\n<p>let&rsquo;s use this dummy plot to get an intuition about how the MSE works.</p>\r\n\r\n<p>the orange line height is the difference between the predicted value and the expected value (blue dot ).</p>\r\n\r\n<p>let&rsquo;s predict Y for these three values of X (3,4,6)</p>\r\n\r\n<p>when<br />\r\nx =3 the predicted value&nbsp;<strong>ŷ = 3.9</strong>&nbsp;while the expected value&nbsp;<strong>y = 2</strong><br />\r\nx =4 the predicted value&nbsp;<strong>ŷ = 4.7</strong>&nbsp;while the expected value&nbsp;<strong>y = 6</strong><br />\r\nx =6 the predicted value&nbsp;<strong>ŷ = 5.8</strong>&nbsp;while the expected value&nbsp;<strong>y = 4</strong></p>\r\n\r\n<p>now we calculate the cost with the MSE formula</p>\r\n\r\n<p>we just have 3 training examples&nbsp;<strong>m = 3</strong></p>\r\n\r\n<p>MSE = 1/ 2* 3 ( (3.9&ndash;2)&sup2; + (4.7&ndash;6)&sup2; + (5.8&ndash;4)&sup2; )</p>\r\n\r\n<p>MSE = 1/6 ( (1.9)&sup2; + (-1.3)&sup2; + ( 1.8)&sup2; ) = 1.42</p>\r\n\r\n<p>of course, if we have a large dataset the cost will be much higher than that number.</p>\r\n\r\n<p>I hope you&rsquo;ve got an intuition about how cost function works, if not then am sure that the little project in the next section will give you a better understanding of the whole process.</p>\r\n\r\n<h2>Weight prediction based on height.</h2>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--2TJpzwC0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j9xoyqpks8ufus8n1zir.png\"><img alt=\"weight and height samples\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--2TJpzwC0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j9xoyqpks8ufus8n1zir.png\" /></a></p>\r\n\r\n<p>I picked 20 samples of weights and heights from a dataset so we can have a better understanding of the linear regression algorithm</p>\r\n\r\n<p>first, we&rsquo;ll&nbsp;<strong>plot the data</strong>&nbsp;then draw a&nbsp;<strong>regression line</strong>&nbsp;to make our predictions and see our model working.</p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--JhSA0zaR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kx8xpeg2okpyl85s6kgb.png\"><img alt=\"weights and heights plot\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--JhSA0zaR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kx8xpeg2okpyl85s6kgb.png\" /></a></p>\r\n\r\n<h2>Regression Line</h2>\r\n\r\n<p>Now we have a plot with height (x-axis)(independent variable) and weight (y-axis)(dependent variable).</p>\r\n\r\n<p>it&rsquo;s time to draw the regression line using the hypothesis equation</p>\r\n\r\n<p>y = &theta;1 + &theta;2. x</p>\r\n\r\n<p>But, how do we choose the &theta;1 and &theta;2 parameters?</p>\r\n\r\n<p>Ok, that&rsquo;s where&nbsp;<strong>Gradient Descent</strong>&nbsp;comes to play, it&rsquo;s a learning algorithm used to choose the best values for parameters to best fit the data, therefore, minimize the cost function (prediction error ).</p>\r\n\r\n<p>we&rsquo;ll talk about gradient descent in another article and for now, let&rsquo;s say we have chosen the optimized parameters to draw the regression line.</p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--PQVRnkcG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6euosyaucpvlb0mnccs4.png\"><img alt=\"Plot with after regression line\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--PQVRnkcG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6euosyaucpvlb0mnccs4.png\" /></a></p>\r\n\r\n<p>and here comes the interesting part, let&rsquo;s make a prediction,<br />\r\nI want our model to predict the weight of a 70 inches, let&rsquo;s do it</p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--bxPuw_gr--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tae5hxrbscrzpqu93kvg.png\"><img alt=\"predict y with regression live\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--bxPuw_gr--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tae5hxrbscrzpqu93kvg.png\" /></a></p>\r\n\r\n<p>it predicted ~135 pounds for 70 inches, if we look in our dataset table you&rsquo;ll find 70.1 inches with 136.4 pounds.<br />\r\nso, we can say that our simple model has good accuracy.</p>\r\n\r\n<h3>Recap</h3>\r\n\r\n<p>In this article, we went through the main idea behind linear regression and how it works.</p>\r\n\r\n<p><strong>Regression</strong>&nbsp;is the process of finding the relation between input and output.</p>\r\n\r\n<p><strong>Linear regression</strong>&nbsp;algorithm estimates that the relation between the dependent and independent variables is linear.</p>\r\n\r\n<p><strong>The hypothesis function</strong>&nbsp;is the straight-line equation used in the machine learning model to map the input to output.</p>\r\n\r\n<p><strong>Cost function</strong>&nbsp;tests the accuracy of the hypothesis function by calculating the average difference between the predicted value and the actual values from the dataset.</p>\r\n','2024-05-14','17:39:35'),(13,'Machine Learning, Neural Networks and Algorithms','<p>Following the previous article about &ldquo;The Core of the Modern Chatbot,&rdquo; this article gives a deeper understanding about the technologies needed for chatbots.</p>\r\n\r\n<h1>Machine Learning</h1>\r\n\r\n<p>NLP (Natural language processing) and Machine Learning are both fields in computer science related to AI (Artificial Intelligence). Machine learning can be applied in many different fields. NLP takes care of &ldquo;understanding&rdquo; the natural language of the human that the program (e.g. chatbot) is trying to communicate with. This understanding enables the program e.g. chatbot) to both interpret input and produce output in the form of human language.</p>\r\n\r\n<p>The machine &ldquo;learns&rdquo; and uses its algorithms through&nbsp;<a href=\"https://www.quora.com/What-is-the-difference-between-supervised-and-unsupervised-learning-algorithms\">supervised and unsupervised learning</a>. Supervised learning means to train the machine to translate the input data into a desired output value. In other words, it assigns an inferred function to the data so that newer examples of data will give the same output for that &ldquo;learned&rdquo; interpretation. Unsupervised learning means discovering new patterns in the data without any prior information and training. The machine itself assigns an inferred function to the data through careful analysis and extrapolation of patterns from raw data. The layers are for analyzing the data in an hierarchical way. This is to extract, with hidden layers, the feature through supervised or unsupervised learning. Hidden layers are part of the data processing layers in a neural network.</p>\r\n\r\n<p><em>Featured CBM:</em>&nbsp;<a href=\"https://chatbotsmagazine.com/building-an-ibm-watson-powered-ai-chatbot-9635290fb1d3\"><em>Building an IBM Watson Powered AI Chatbot</em></a></p>\r\n\r\n<p><strong>Neural Networks</strong><br />\r\nNeural networks are one of the learning algorithms used within machine learning. They consist of different layers for analyzing and learning data.</p>\r\n\r\n<p><a href=\"https://res.cloudinary.com/practicaldev/image/fetch/s--xBsOdvHx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/095uidcrsfurjaaa4rqa.png\"><img alt=\"\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--xBsOdvHx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/095uidcrsfurjaaa4rqa.png\" /></a><br />\r\nHidden learning layers and neurons by Nvidia</p>\r\n\r\n<p>Every hidden layer tries to detect patterns on the picture. When a pattern is detected the next hidden layer is activated and so on. The picture of the Audi A7 above illustrates this perfectly. The first layer detects edges. Then the following layers combine other edges found in the data, ultimately a specified layer attempts to detect a wheel pattern or a window pattern. Depending on the amount of layers, it will be or not be able to define what is on the picture, in this case a car.The more layers in a neural network, the more is learned and the more accurate the pattern detection is. Neural Networks learn and attribute weights to the connections between the different neurons each time the network processes data. This means the next time it comes across such a picture, it will have learned that this particular section of the picture is probably associated with for example a tire or a door.</p>\r\n\r\n<p><em>Featured CBM:</em>&nbsp;<a href=\"https://chatbotsmagazine.com/unsupervised-deep-learning-for-vertical-conversational-chatbots-c66f21b1e0f\"><em>Unsupervised Deep Learning for Vertical Conversational Chatbots</em></a></p>\r\n\r\n<h1>Machine Learning Algorithms</h1>\r\n\r\n<p>This chapter shows some of the most important machine learning algorithms, more information about algorithms can be found via the following links.&nbsp;<a href=\"https://www.iflexion.com/blog/machine-learning-new-gold-rush/\">[1]</a><a href=\"http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/\">[2]</a><a href=\"https://datafloq.com/read/12-algorithms-every-data-scientist-should-know/2024#datascience\">[3]</a></p>\r\n\r\n<p><strong>Decision Tree Algorithms</strong><br />\r\nIn this algorithm a decision tree is used to map decisions and their possible consequences, including chances, costs and utilities. This method allows the problem to be approached logically and stepwise to get to the right conclusion. An important algorithm that evolved from this algorithm is the Random Tree algorithm. This algorithm uses multiple trees to avoid overfitting that often occurs with using decision trees.</p>\r\n\r\n<p><strong>Bayesian Algorithms</strong><br />\r\nApplies Bayesian theorem for regression and classification problems involved with probability. It attempts to show the probabilistic relationship between different variables and determine, given the variables, which category it more likely belongs to.</p>\r\n\r\n<p><strong>Regression Algorithms</strong><br />\r\nWell suited to statistical machine learning, regressions seek to model the relationship between variables. By observing these relationships you aim to establish a function that more or less mimics this relationship. This mean that when you observe more variables you can say with some confidence and with a margin of error, where they may lay along the function.</p>\r\n\r\n<p><strong>Support Vector</strong><br />\r\nThe support vector algorithm is used in the grouping of points on a dimensional plane. The grouping is done by creating a hyperplane that separates the groups with a margin that is as wide as possible. This helps with the classification and is used for example in advertising or human RNA splicing.</p>\r\n\r\n<p><strong>Ensemble Methods</strong><br />\r\nEnsemble methods combine various weaker supervised learning algorithms. A combination of very different models will usually produce better results. By combining the various methods you can handle bias with certain models, reduce the variance and reduce overfitting by averaging it out more.</p>\r\n\r\n<p><strong>Clustering Algorithms</strong><br />\r\nThe main purpose of this algorithm is to cluster the available data into groups, where the data points in such a group are more similar to each other than those in other groups. The more important clustering methods are hierarchical, centroid, distribution and density.</p>\r\n\r\n<p><strong>Association Rule Learning Algorithms</strong><br />\r\nThis is about the rules that can be established between the itemsets and the transactions for these items and item sets. The relation between X and Y, thus the probability of when you obtain X you also obtain Y. This rule is found in the database by observing the itemsets and the items therein.</p>\r\n\r\n<p><strong>Artificial Neural Network Algorithms</strong><br />\r\nArtificial Neural Network algorithms are inspired by the human brain. The artificial neurons are interconnected and communicate with each other. Each connection is weighted by previous learning events and with each new input of data more learning takes place. A lot of different algorithms are associated with Artificial Neural Networks and one of the most important is Deep learning. An example of Deep Learning can be seen in the picture above. It is especially concerned with building much larger complex neural networks.</p>\r\n\r\n<p><strong>Dimensionality Analysis Algorithms</strong><br />\r\n<a href=\"https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/\">Dimensionality</a>&nbsp;is about the amount of variables in the data and the dimensions they belong to. This type of analysis is aimed at reducing the amount of dimensions with the associated variables while at the same time retaining the same information. In other words it seeks to remove the less meaningful data while at the same time ensuring the same end result.</p>\r\n\r\n<p>This concludes our 3rd part in the chatbot series, if you&rsquo;d like to share your opinion and have any feedback or suggestions please do so in the comment section or through email. You can reach me at&nbsp;<a href=\"mailto:henk.pelk@linkit.nl\">henk.pelk@linkit.nl</a>.</p>\r\n','2024-05-14','17:40:18'),(14,'Manual Testing','<p>Manual testing is a software testing where testers manually test the software or application quality. Testers interact with the system like how an end user would do to identify bugs, defects, and issues in the software that create friction in user experience. It is great for small-scale testing and even for large-scale testing where there are thousands and millions of items and features to test, manual testing is still needed to some extent.</p>\r\n\r\n<p>During the software development lifecycle, when the application is still in build process, automation testing may not be practical or detailed enough. In such cases, manual testing is incredibly valuable as it offers a perspective that can only come from a human tester. This approach is especially useful for evaluating user interface (UI), user experience (UX), and overall software functionality from an end-user&#39;s viewpoint.<br />\r\nMastering manual testing is crucial for both experienced professionals and beginners to ensure that application functions correctly and provides a seamless user experience.</p>\r\n\r\n<p>Types of Manual Testing:</p>\r\n\r\n<ol>\r\n	<li>Exploratory testing</li>\r\n	<li>Ad-hoc testing</li>\r\n	<li>Usability testing</li>\r\n	<li>Functional Testing</li>\r\n	<li>Non-functional Testing</li>\r\n	<li>Unit Testing</li>\r\n	<li>UI Testing</li>\r\n	<li>Integration Testing</li>\r\n</ol>\r\n\r\n<p>Manual testing process:</p>\r\n\r\n<ol>\r\n	<li>Understanding the requirements specifications.</li>\r\n	<li>Test plan development.</li>\r\n	<li>Test scenarios and case creation.</li>\r\n	<li>Setting up the test environment.</li>\r\n	<li>Executing test cases.</li>\r\n	<li>Logging defects.</li>\r\n	<li>Analysing results and reporting.</li>\r\n	<li>Retesting and Regression testing.</li>\r\n</ol>\r\n\r\n<p>Advantages of manual testing:</p>\r\n\r\n<ul>\r\n	<li>\r\n	<p>Human insight and intuition<br />\r\n	Testers can use their experience and intuition to explore and evaluate the software. This human touch is beneficial in comprehending the user experience, pinpointing UI/UX issues, and providing feedback on the subjective aspects of the software, such as its appearance and feel.</p>\r\n	</li>\r\n	<li>\r\n	<p>Flexibility and adaptability<br />\r\n	Manual testing can be done in early stages of development, so testers can make a quick changes in their testing approach as and when the software evolves. Testers can easily adapt their test cases and approaches if there is any change in code or application.</p>\r\n	</li>\r\n	<li>\r\n	<p>Cost-effectiveness for small-scale projects<br />\r\n	Manual testing can be more cost-effective for smaller projects or those with limited budgets.</p>\r\n	</li>\r\n	<li>\r\n	<p>Real-world application<br />\r\n	In mobile application testing it is necessary to make sure the feel of the application, UI, gesture controls, and device-specific features are best evaluated manually.</p>\r\n	</li>\r\n</ul>\r\n\r\n<p>Limitations of manual testing:</p>\r\n\r\n<ul>\r\n	<li>\r\n	<p>Time consumption and resource intensity<br />\r\n	Manual testing can be significantly more time-consuming than automated testing, especially for large and complex applications. It requires more human resources, and the time taken to execute test cases manually can lead to longer development cycles.</p>\r\n	</li>\r\n	<li>\r\n	<p>Limitations in handling complex or large-scale testing scenarios<br />\r\n	When dealing with complex or large-scale applications, manual testing may not be effective as testing every possible scenario or combination is infeasible, leading to gaps in test coverage.</p>\r\n	</li>\r\n	<li>\r\n	<p>Potential for human error<br />\r\n	Given the repetitive and sometimes monotonous nature of manual testing, there is a higher risk of human error. Testers might overlook bugs or inconsistencies, especially when dealing with extensive testing scenarios.</p>\r\n	</li>\r\n	<li>\r\n	<p>Less effective in regression or performance testing<br />\r\n	There are better approaches than manual testing for situations requiring regression testing performance testing. In these cases, automated testing is often more efficient and reliable.</p>\r\n	</li>\r\n</ul>\r\n\r\n<p>Examples to Highlight importance of Manual Testing:</p>\r\n\r\n<p>Example 1: In exploratory testing, testers should explore and interact with the application deeply before deep diving into the details and it makes uncover bugs that they don&#39;t even know of in the first place. It also makes the tester to know more about the application in detail. This knowledge will help them to create more efficient test cases.</p>\r\n\r\n<p>Example 2: Initial requirement of the project can be very simple: to test the login functionality if it accepts valid username and password, but it may change in between to includes password strength verification. When doing the manual testing, testers only have to type in several combinations of passwords that meet and do not meet the password strength criteria to see how the system responds.</p>\r\n\r\n<p>Example 3: If we take an application like Whatsapp, it is necessary for tester to make sure the application is very handy to use, since in real world user will handle the application manually.</p>\r\n','2024-05-14','17:43:19'),(15,'Big Data from the Ground up','<h2>First thing first, what is&nbsp;<em>Not</em>&nbsp;Big Data?</h2>\r\n\r\n<p>Basically, everything that can fit into one computer/machine. How did we work with data before Big Data? Relational DataBases(RDB) were extremely popular (and are still heavily in use).</p>\r\n\r\n<p>What is a Relational DB?</p>\r\n\r\n<blockquote>\r\n<p>&quot; A relational database is a digital database based on the relational model of data. It is a set of formally described tables from which data can be accessed or reassembled in many different ways without having to reorganize the database tables.&quot; -&nbsp;<a href=\"https://en.wikipedia.org/wiki/Relational_database\">Wikipedia</a>.</p>\r\n</blockquote>\r\n\r\n<p>Examples for RDB are MySql and SqlServer.</p>\r\n\r\n<p>In RDB the data is being modeled using relations. The goal is to have as little to no data duplications as possible at the price of creating many tables and reference links. This is data normalization. Data normalization has many rules for what we need to do in order to normalize the data.<br />\r\nThe Rules are First Normal Form( 1NF), 2NF, 3NF, 4NF, and BCNF - Boyce and Codd Normal Form. To learn more about data normalization,&nbsp;<a href=\"https://support.microsoft.com/en-us/help/283878/description-of-the-database-normalization-basics?WT.mc_id=devto-blog-adpolak\">click here</a>.</p>\r\n\r\n<p>Although SQL is a well-structured query language, it&#39;s not the only tool to handle data today. We will go over why, but first, let&#39;s hold an example in mind:</p>\r\n\r\n<p>We are building a social media site. Like Twitter.Our site holds data that translates into many relations(tables). Some of our main screens are being constructed from the next questions:</p>\r\n\r\n<p><em>-&gt; How many tweets liked by a specific person?</em><br />\r\n<em>-&gt; How can I see my personal twitter feed?</em></p>\r\n\r\n<p>These questions are the main blocks of our business requirements: Our users would like to have access to tweets from their friends or the people that they follow. They want to see the count of likes, count of shares and comments. *<em>How can we design an SQL query to get that data? *</em>&nbsp;We can look at the following of each of our users, do a join operation on recent tweets, join operation on likes, another join operation on retweet with join operation on comments and so on.</p>\r\n\r\n<ul>\r\n	<li><strong>join</strong>&nbsp;- this is a SQL join operator, where we combine columns from one or more tables in RDB according to a specific key. This is only one option on how we can query the data to answer the main business question.</li>\r\n</ul>\r\n\r\n<p>Now, to a more focused business question:<br />\r\n<br />\r\n<em>-&gt; How many likes one specific user-generated?</em></p>\r\n\r\n<p>How do we answer this question? we can leverage SQL and create a query for that as well. To sum it up, we can develop a whole social media platform using SQL. But, wait, we didn&#39;t talk about Scale and the size of the queried data. According to analysts, we are facing 100 of millions of tweets a day. Meaning it won&#39;t fit in one machine. On top of that, we now face new requirements: Provide results super fast. What super fast mean? in a matter of few milliseconds. Whoops. Now a query with 5-10 joins that takes more than 1, 2 or 5 sec to finish is not good enough (according to today&#39;s business needs).</p>\r\n\r\n<p>Now that we understand that RDB isn&#39;t always the answer, let&#39;s look at what is Big Data.</p>\r\n\r\n<h2>So, what is Big Data?</h2>\r\n\r\n<p>It&#39;s a large set of data that can&#39;t fit into one machine.</p>\r\n\r\n<h3>Two main challenges:</h3>\r\n\r\n<ol>\r\n	<li>More Data to process ( then can fit in one machine )</li>\r\n	<li>Faster answer/query response requirements</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--YW0OYbXf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://media1.tenor.com/images/c89d62dcabc0fc27b259c15c64fa2cc8/tenor.gif%3Fitemid%3D6056405\" style=\"height:272px; width:430px\" /></p>\r\n\r\n<p>One of the techniques to work with a large set of data is NoSQL. Where in most scenarios there are no relations at all. This approach can help us get the results faster, as well as store more data.</p>\r\n\r\n<p>NoSQL is one approach to how we can solve the challenges of today&#39;s data-driven systems.<br />\r\nHow do we work with data with Big Data constraints? We first think about how we are going to use that data. What are the business cases that we would like to answer? How we will store and later query the data? There is a price for working with Big Data and being fast. One of them might just be giving up on&nbsp;<strong>query flexibility</strong>.</p>\r\n\r\n<p>We should know in advance which queries we need to serve to answer the business question. We will prepare in advance for queries - by duplicating the data in various ways to gain faster reads at the price of slower writes. This process is called&nbsp;<a href=\"https://en.wikipedia.org/wiki/Denormalization\">Denormalize</a>&nbsp;data.</p>\r\n\r\n<blockquote>\r\n<p>&quot; Denormalization is the process of trying to improve the read performance of a database, at the expense of losing some write performance, by adding redundant copies of data or by grouping data in advance.&quot; -Wikipedia.</p>\r\n</blockquote>\r\n\r\n<p>That brings us to a new challenge - data design challenge:</p>\r\n\r\n<p>How to organize the data so it will be easy to query it fast?</p>\r\n\r\n<p>Let&#39;s go back to our example. How many tweets are liked by a specific user? For solving that, we will keep a counter and update it on every &#39;like&#39; as part of the write operation. That will save us the cost of calculating the count at reading time.<br />\r\nBut, once we go with that approach, we need to build a data version for every specific question/business case. We become fast but lose flexibility. Another approach is to write a row for every &#39;like&#39; operation and calculate it once every 5 minutes, to keep the result &#39;5 minutes fresh&#39;. That brings us to new storage solutions: NoSql, DocumentDB, GraphDB and more. All the above solutions, need to support a load of many queries at high scale while not getting stuck in transactions with read/write locks.</p>\r\n\r\n<p><a href=\"https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock\">Read/write locks</a>&nbsp;are synchronization methods to solve readers-writers problems and usually means that multiple threads can read the data in parallel but only a thread with the exclusive lock can update the data.</p>\r\n\r\n<p>This is why read/write locks have the potential of getting us stuck by locking part of the data for unknown time or TTL (Time To Live - also used as the max threshold to how much time the resource will be locked).</p>\r\n\r\n<p>This concludes the essence of Big Data challenges, which brings us to Big Data frameworks. Today, many companies will try to store the data in a less processed manner, raw data, without losing any data and not filtering it. Of course with addressing information privacy laws such as GDPR, HIPAA and more- according to requirements.</p>\r\n\r\n<p>This is why we want to leverage various frameworks to process data at scale, fast. Let&#39;s look at the next approach that is built on the idea that we can capture snapshots of the data and just read them when necessary.</p>\r\n\r\n<p><strong>Building blocks for designing software architecture to support data at scale:</strong></p>\r\n\r\n<ol>\r\n	<li>Data Lake</li>\r\n	<li>Data Warehouse</li>\r\n	<li>Processing Layer</li>\r\n	<li>UI / API</li>\r\n</ol>\r\n\r\n<h3>Data Lake</h3>\r\n\r\n<p>A data lake is where we save all the data in a raw manner, without processing it. We save it in that manner so later, when business cases arise, or new products, we will have it relatively available and in raw format. This storage should be cheap as we would like to store everything we can( with data compliance in mind). The cloud allows us to store everything. On&nbsp;<a href=\"https://azure.microsoft.com/en-us/services/storage/blobs/?WT.mc_id=devto-blog-adpolak\">Azure</a>, we can store everything in blobs storage. On AWS, we can use s3. We use the blobs as an endless file system. The data that we store in Data Lake will probably have no meaning/purpose at the start and only stored so we will not lose data or business opportunity revolving it later.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--p_wIrgxz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://media1.tenor.com/images/36ee1754c41f0be3e27d7247111061b3/tenor.gif%3Fitemid%3D14656533\" style=\"height:640px; width:640px\" /></p>\r\n\r\n<h3>Data Warehouse (DW)</h3>\r\n\r\n<p>A data warehouse will hold more processed data, perhaps with a specific structure for fast query or fast processing. DW will be highly used by Data Scientists, Business Analysts, and Developers. DW will have indexes for fast accessing and data will be saved in a unique/optimized manner. Think about it like a library with an index, where you don&#39;t need to go over all the books to find the book you want.</p>\r\n\r\n<p><em>Up until now, we discussed Data Lake and Data Warehouse. Data that reached DW already has a purpose and is used for answering business questions. Meaning, that data will have at least two duplications, one in raw format and more copies in processed format saved at the DW.</em></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--Yoa8Gz5D--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://media.tenor.com/images/47b750ba05e40f3b144c3cda10bf5c66/tenor.gif\" style=\"height:90px; width:212px\" /></p>\r\n\r\n<h3>Processing Layer</h3>\r\n\r\n<p>When working with data, we will want tools that help us to analyze, systematically extract information from, create machine learning models and more. This is the responsibility of the processing layer. We will do it in a distributed manner.<br />\r\nThere is a whole ecosystem for Distributed Processing, one of the known frameworks is Apache Spark:</p>\r\n\r\n<blockquote>\r\n<p>Apache Spark is part of the distributed processing ecosystem. It has built-in modules for Batch, Streaming, SQL, Machine Learning and Graph processing.</p>\r\n</blockquote>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--Fou5hC0J--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://media1.tenor.com/images/b228138ca189b63989d295492e8a8b16/tenor.gif%3Fitemid%3D4774531\" style=\"height:213px; width:498px\" /></p>\r\n\r\n<h3>UI / API</h3>\r\n\r\n<p>Many times we will want to build an interface to serve the business, It can be a UI(User Interface) or API(Application programming interface) that will serve our clients. Our clients will usually want insights on the data. It can be in the shape of a twitter feed. This is the last layer of our basic generic architecture that will deliver the outcome to our clients.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--5FTWxxep--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_800/https://media1.tenor.com/images/74a535813850aae5ef9e53c7c1c2b03c/tenor.gif%3Fitemid%3D14098332\" style=\"height:480px; width:640px\" /></p>\r\n\r\n<hr />\r\n<p>? Summary ?<br />\r\nLet&#39;s wrap it up. These building blocks open a whole new world of workload management, various types of files (Json, Binary, Parquet, TDMS, MDF4 and etc), log systems, new architectures specifically designed to handle scale and more.<br />\r\nToday there are many ways to describe big data. But, in the end, we need to know and understand the tools in order to optimize our work and design and implement the best solution to fit our business requirements.</p>\r\n\r\n<hr />\r\n<h2>Do you work with&nbsp;<strong>Big Data in Production?</strong></h2>\r\n\r\n<p>Share your best practices, ideas, and questions in the comments!</p>\r\n','2024-05-14','17:44:54'),(16,'Guide to exploring data in Python','<p>Data professionals rely on Exploratory Data Analysis (EDA) to understand the data and how variables within the data are related. There are various tools used when performing EDA but the key of them all is visualization. Through visualizations, we can easily see how the data looks and we can make assumptions that will guide how we will analyze the data.</p>\r\n\r\n<p>We will use&nbsp;<a href=\"https://colab.research.google.com/drive/15Va8tUmaJJfkv3KSLc4ku_F-87AdApdL?usp=sharing\">Google Colab</a>&nbsp;for this demonstration to show that you do not need to download Python software locally to uncover insights in your data. Google Colab is a powerful platform that allows you to write and execute your Python code in your browser and hence convenient for your data analysis needs.</p>\r\n\r\n<h2>Core EDA libraries in Python</h2>\r\n\r\n<p>Python has numerous libraries tailored for manipulating and analyzing data. Below are some of the libraries that you will need for your EDA:</p>\r\n\r\n<ul>\r\n	<li><strong>Pandas</strong>&nbsp;- This library helps in loading the data and cleaning the data</li>\r\n	<li><strong>Numpy</strong>&nbsp;- This library helps when performing numerical computations in Python. Numpy works with pandas and it is good for manipulating big datasets</li>\r\n	<li><strong>Matplotlib</strong>&nbsp;and&nbsp;<strong>Seaborn</strong>&nbsp;are for visualizing the data</li>\r\n</ul>\r\n\r\n<h3>Loading the libraries</h3>\r\n\r\n<pre>\r\n<code>import pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n</code></pre>\r\n\r\n<h3>Loading the data</h3>\r\n\r\n<p>The data for this demonstration was sourced from Milwaukee City datasets for 2023.</p>\r\n\r\n<pre>\r\n<code>property_df = pd.read_csv(&#39;/content/armslengthsales_2023_valid.csv&#39;)\r\n</code></pre>\r\n\r\n<h3>Overview of the data</h3>\r\n\r\n<p>Head: This function shows the top rows of the data</p>\r\n\r\n<pre>\r\n<code>property_df.head()\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9exf4106uo0fksrsl0cp.png\"><img alt=\"Snapshot of the data\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9exf4106uo0fksrsl0cp.png\" style=\"height:355px; width:800px\" /></a></p>\r\n\r\n<p>This is an incomplete snap of the data. (there are many rows in the data and hence could not catpure the whole tablel).</p>\r\n\r\n<p><strong>Shape</strong>&nbsp;- This shows the number of rows and columns in the data</p>\r\n\r\n<pre>\r\n<code>property_df.shape\r\n</code></pre>\r\n\r\n<p>(5831, 20)<br />\r\nThe output (5831, 20) shows that there are 5831 rows and 20 columns</p>\r\n\r\n<p><strong>Data types</strong>&nbsp;- This shows the data types of the variables in the dataset.</p>\r\n\r\n<pre>\r\n<code>property_df.dtypes\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvlxaeo9wx5q4kl4lq61n.png\"><img alt=\"Data Types in the data\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvlxaeo9wx5q4kl4lq61n.png\" style=\"height:394px; width:446px\" /></a></p>\r\n\r\n<p>From the output above, we can see that our datasets have the data types int, float, and object. We can see from the output that we have 6 categorical variables (object) and 14 numerical (int64 and float64) variables.</p>\r\n\r\n<h3>Missing values</h3>\r\n\r\n<p>Another important part of EDA analysis is checking for missing values. Missing values are unknown, unspecified, or unrecorded values in the dataset. In Pandas, the missing values are usually represented with NaN.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgxz6wjfulp3o7t1hkeyx.png\"><img alt=\"Missing Values\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgxz6wjfulp3o7t1hkeyx.png\" style=\"height:224px; width:800px\" /></a></p>\r\n\r\n<p>From the table above, we can see that the columns CondoProject, Rooms, and Bdrms have missing values represented by NaN values.</p>\r\n\r\n<p>The best way to see the null values in your dataset is by using the .info command:</p>\r\n\r\n<pre>\r\n<code>property_df.info()\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fz605qcbh5te6nzu7z063.png\"><img alt=\"Null Values\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fz605qcbh5te6nzu7z063.png\" style=\"height:467px; width:419px\" /></a></p>\r\n\r\n<p>We can see from the above that while the total rows are 5831, some columns do not have 5831 rows. Some of the variables with missing values are CondoProject, Style, Extwall, Stories, Year_Built, Rooms, FinishedSqft and Bdrms. Let&rsquo;s check the missing values in each column</p>\r\n\r\n<pre>\r\n<code>property_df.isna().sum()\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fd3edyjtnfarpihvd17kt.png\"><img alt=\"missing Values in columns\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fd3edyjtnfarpihvd17kt.png\" style=\"height:381px; width:412px\" /></a></p>\r\n\r\n<p>When faced with a variable with a big proportion of missing values, we can drop the affected column. For those that have fewer missing values, we can drop the rows or use estimates to replace the missing values.</p>\r\n\r\n<h3>Dealing with the column with the most missing values</h3>\r\n\r\n<p>From the previous tables, we saw that the&nbsp;<em>CondoProject</em>&nbsp;variable has more than 80% missing values. The best way we can deal with this variable is dropping it in its entirety as done below.</p>\r\n\r\n<pre>\r\n<code>property_df.drop(columns=&#39;CondoProject&#39;, axis=1, inplace=True)\r\n</code></pre>\r\n\r\n<p>For the remaining variables with missing values with small proportion of missing values, we can just drop the respective rows that have missing values.</p>\r\n\r\n<pre>\r\n<code>property_df = property_df.dropna()\r\n</code></pre>\r\n\r\n<p>After dropping the CondoProject column and the rows with null values, we can see that the total rows have dropped to 4690 from the initial 5831 and that we have 19 columns instead of the initial 20.</p>\r\n\r\n<p>The outcome is:<br />\r\n<a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7daf1oeta0k0pwuiddjf.png\"><img alt=\"Dropped missing values\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7daf1oeta0k0pwuiddjf.png\" style=\"height:437px; width:473px\" /></a><br />\r\nNow, we have clean data and we can now perform data visualization.</p>\r\n\r\n<h3>Summary statistics</h3>\r\n\r\n<pre>\r\n<code>property_df.describe().T\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwndu8qd9w1aalkmixx8t.png\"><img alt=\"Summary statistics\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwndu8qd9w1aalkmixx8t.png\" style=\"height:398px; width:722px\" /></a><br />\r\nThe summary statistics show the minimum, maximum, first quartile, third quartile, mean, and maximum values for each variable in the dataset.</p>\r\n\r\n<h3>Univariate variables</h3>\r\n\r\n<p><strong>1. Histogram of Year Built variable</strong></p>\r\n\r\n<pre>\r\n<code>sns.histplot(property_df[&#39;Year_Built&#39;])\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fev8oqajf7xjo7g2ovpvc.png\"><img alt=\"Year Built distribution\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fev8oqajf7xjo7g2ovpvc.png\" style=\"height:427px; width:622px\" /></a></p>\r\n\r\n<p>We can see from the histogram that most of the houses were built in the 1950s and 1920s. Since the 1980s, the number of houses built in Milwaukee has been declining.</p>\r\n\r\n<p><strong>2. Distribution Rooms variable</strong></p>\r\n\r\n<pre>\r\n<code>sns.histplot(property_df[&#39;Rooms&#39;])\r\nplt.title(&ldquo;Distribution of Rooms Variable&rdquo;)\r\n\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fexzycjaqzygp3w8l9v7h.png\"><img alt=\"Rooms variable distribution\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fexzycjaqzygp3w8l9v7h.png\" style=\"height:455px; width:580px\" /></a></p>\r\n\r\n<p>From the histogram of the rooms variable, we can conclude that many properties have between 5 and 12 rooms. Only a few properties have more than 20 rooms.<br />\r\n<strong>3. Distribution of stories variable</strong></p>\r\n\r\n<pre>\r\n<code>sns.histplot(property_df[&#39;Stories&#39;])\r\nplt.title(&ldquo;Distribution of Stories variable&rdquo;)\r\n\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgjvb5usnn368gbhcbey7.png\"><img alt=\"Stories variable distribution\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgjvb5usnn368gbhcbey7.png\" style=\"height:455px; width:580px\" /></a></p>\r\n\r\n<p>Most of the properties are between 1 and 2 stories tall. There are a few properties that have between 2.5 and 4 stories.<br />\r\n<strong>4. Distribution of Sales Price variable</strong></p>\r\n\r\n<pre>\r\n<code>sns.histplot(property_df[&#39;Sale_price&#39;])\r\nplt.title(&quot;Distribution of Sales Price&quot;)\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frvgg2222rvvu6f4h9gu1.png\"><img alt=\"Sales price distribution\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frvgg2222rvvu6f4h9gu1.png\" style=\"height:455px; width:571px\" /></a><br />\r\nFrom the plot above, we can see that most of the properties are concentrated between around $15000 and $500,000. Other properties cost more than $1,000,000 but they are few.<br />\r\n<strong>5. Property style distribution</strong></p>\r\n\r\n<pre>\r\n<code>style_count = property_df[&#39;Style&#39;].value_counts()\r\norder = style_count.index\r\nplt.figure(figsize=(12, 6))\r\nsns.barplot(x=style_count.index, y=style_count.values, order=order, palette=&#39;viridis&#39;)\r\nplt.ylabel(&#39;Frequency&#39;)\r\nplt.title(&#39;Frequency of property styles&#39;)\r\nplt.xticks(rotation=45)\r\nplt.show()\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74pv2lgnin3w9fx2n0l9.png\"><img alt=\"Property styles\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74pv2lgnin3w9fx2n0l9.png\" style=\"height:608px; width:800px\" /></a></p>\r\n\r\n<p>The frequency plot above shows that the most common property styles in Milwaukee are Ranch and Cape Cod while the least popular property styles are Office and Store buildings.<br />\r\n<strong>6. Property type distribution</strong></p>\r\n\r\n<pre>\r\n<code>sns.histplot(property_df[&#39;PropType&#39;])\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyie69k1oeq84opxv4nzw.png\"><img alt=\"Property Type\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyie69k1oeq84opxv4nzw.png\" style=\"height:432px; width:580px\" /></a><br />\r\nWe can see that the most common property type is residential property.</p>\r\n\r\n<h3>Bivariate variables</h3>\r\n\r\n<p><strong>Scatterplot for finished square feet and sales price</strong></p>\r\n\r\n<pre>\r\n<code>plt.figure(figsize=(8, 6))\r\nsns.scatterplot(x=&#39;FinishedSqft&#39;, y=&#39;Sale_price&#39;, data=property_df)\r\nplt.title(&#39;Relationship between Finished Sqft and Sale Price&#39;)\r\nplt.xlabel(&#39;Finished Sqft&#39;)\r\nplt.ylabel(&#39;Sale Price&#39;)\r\nplt.show()\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fhsjligjag1v8h8fk23oh.png\"><img alt=\"Sales price and finished sqft\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fhsjligjag1v8h8fk23oh.png\" style=\"height:547px; width:700px\" /></a><br />\r\nSales Price and Finished Sqft have a positive linear relationship. An increase in Finished Sqft leads to an increase in the Sales Price.</p>\r\n\r\n<h3>Multivariate</h3>\r\n\r\n<p><strong>Correlation plot</strong></p>\r\n\r\n<pre>\r\n<code>#computing correlation matrix\r\ncorr_matrix = property_df.select_dtypes(include=&#39;number&#39;).drop(columns=[&#39;PropertyID&#39;, &#39;taxkey&#39;, &#39;District&#39;]).corr()\r\n\r\n# Plotting the heatmap of correlation matrix\r\nplt.figure(figsize=(12, 10))\r\nsns.heatmap(corr_matrix, annot=True, cmap=&#39;coolwarm&#39;, fmt=&#39;.2f&#39;, linewidths=0.5)\r\nplt.title(&#39;Correlation Matrix of Numerical Variables&#39;)\r\nplt.show()\r\n</code></pre>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9tgtlq7gogte5iatjezw.png\"><img alt=\"Correlation plot\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9tgtlq7gogte5iatjezw.png\" style=\"height:730px; width:800px\" /></a><br />\r\nThe correlation matrix above shows us how the variables are related. For instance, we can see that Rooms and Bedrooms variables are highly correlated with a correlation of 0.86.</p>\r\n\r\n<h3>Conclusion</h3>\r\n\r\n<p>This comprehensive guide has shown you the fundamental steps that we use when exploring data. Throughout this tutorial, you have learned to load and have an overview of your dataset, handle missing values, perform both univariate and bivariate analysis, and finally examine multivariate relationships using correlation analysis.</p>\r\n\r\n<p>From this analysis, we have learned some valuable insights on houses in Milwaukee city like pricing range, property sizes, and architectural styles. These insights that we have uncovered highlight the powerfulness of EDA and why every data practitioner should be good at it. This guide has given you a good foundation to keep exploring and visualizing your data. Continue exploring your data to unlock more insights!</p>\r\n\r\n<h3>Additional Resources</h3>\r\n\r\n<ol>\r\n	<li><a href=\"https://www.analyticsvidhya.com/blog/2022/07/step-by-step-exploratory-data-analysis-eda-using-python/\">https://www.analyticsvidhya.com/blog/2022/07/step-by-step-exploratory-data-analysis-eda-using-python/</a></li>\r\n	<li><a href=\"https://www.geeksforgeeks.org/exploratory-data-analysis-in-python/\">https://www.geeksforgeeks.org/exploratory-data-analysis-in-python/</a></li>\r\n	<li><a href=\"https://www.youtube.com/watch?v=Liv6eeb1VfE\">https://www.youtube.com/watch?v=Liv6eeb1VfE</a></li>\r\n</ol>\r\n','2024-05-14','17:46:37'),(18,'Install NVTOP in WSL debian','<p><a href=\"https://github.com/Syllo/nvtop\">NVTOP</a>&nbsp;is like htop but for your graphics module . In this short tutorial I will share how to install nvtop in wsl debian</p>\r\n\r\n<p>OS used :</p>\r\n\r\n<p><code>cat /etc/os-release</code></p>\r\n\r\n<pre>\r\n<code>PRETTY_NAME=&quot;Debian GNU/Linux 12 (bookworm)&quot;\r\nNAME=&quot;Debian GNU/Linux&quot;\r\nVERSION_ID=&quot;12&quot;\r\nVERSION=&quot;12 (bookworm)&quot;\r\nVERSION_CODENAME=bookworm\r\nID=debian\r\nHOME_URL=&quot;https://www.debian.org/&quot;\r\nSUPPORT_URL=&quot;https://www.debian.org/support&quot;\r\nBUG_REPORT_URL=&quot;https://bugs.debian.org/&quot;\r\n</code></pre>\r\n\r\n<p>1) Verify your graphics driver<br />\r\nIn my case I have nvidida graphics</p>\r\n\r\n<pre>\r\n<code>nvidia-smi\r\n</code></pre>\r\n\r\n<p>It should display details about your graphics , if not first install it</p>\r\n\r\n<p>2) Edit your debian source list to include non-free releases</p>\r\n\r\n<pre>\r\n<code>sudo nano /etc/apt/sources.list\r\n</code></pre>\r\n\r\n<p>2) Modify the source list and add following</p>\r\n\r\n<pre>\r\n<code>deb http://deb.debian.org/debian/ bookworm main contrib non-free-firmware\r\n</code></pre>\r\n\r\n<p>3) Get update</p>\r\n\r\n<pre>\r\n<code>sudo apt get update\r\n</code></pre>\r\n\r\n<p>4) Now you can install nvtop</p>\r\n\r\n<pre>\r\n<code>sudo apt install nvtop\r\n</code></pre>\r\n\r\n<p>Now hit&nbsp;<code>nvtop</code>&nbsp;and you are all set !!</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzix26djow1vmzccplctj.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzix26djow1vmzccplctj.png\" style=\"height:484px; width:800px\" /></a></p>\r\n','2024-05-14','17:51:07'),(19,'AI & Kubernetes','<p>Just like so many in the tech industry, Artificial Intelligence (AI) has come to the forefront in my day-to-day work. I&#39;ve been starting to learn about how &quot;AI&quot; fits into the world of Kubernetes - and vice versa. This post will start a series where I explore what I&#39;m learning about AI and Kubernetes.</p>\r\n\r\n<h3><strong>Types of AI Workloads on Kubernetes</strong></h3>\r\n\r\n<p>To describe the AI workloads engineers are running on Kubernetes, we need some terminology. In this post I&rsquo;m going to describe two major types of workloads:&nbsp;<strong>training</strong>&nbsp;and&nbsp;<strong>inference</strong>. Each of these two terms describes a different aspect of the work Platform Engineers do to bring AI workloads to life. In this post, I&rsquo;ll highlight two roles in the path from concept to production for AI workloads. Platform Engineers bridge the gap between Data Scientists who design models, and the end users who interact with trained implementations of the models those Data Scientists designed.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmdpl7i2kp726wgr7xhdp.png\"><img alt=\"A Data Scientist (woman in an orange shirt, white lab coat, and gray pants) sits at a drafting table working on blueprints for a robot. A Platform Engineer (a man in a teal sweater-vest, white long sleeved shirt, and brown pants) works on building the robot from the Data Scientist\'s blueprint.\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmdpl7i2kp726wgr7xhdp.png\" style=\"height:421px; width:800px\" /></a></p>\r\n\r\n<p><em>Data Scientists design models while Platform Engineers have an important role to play in making them run on hardware.</em></p>\r\n\r\n<p>There&#39;s a lot of work that happens before we get to the stage of running an AI model in production. Data scientists choose the model type, implement the model (the structure of the &quot;brain&quot; of the program), choose the objectives for the model, and likely gather training data. Infrastructure engineers manage the large amounts of compute resources needed to train the model and to run it for end users. The first step between designing a model and getting it to users, is training.</p>\r\n\r\n<p><em>Note: AI training Workloads are generally a type of Stateful Workload, which you can<a href=\"https://kaslin.rocks/stateful-kubernetes/\">&nbsp;learn more in my post about them.</a></em></p>\r\n\r\n<p><strong>Training Workloads</strong></p>\r\n\r\n<p>&quot;Training&quot; a model is the process for creating or improving the model for its intended use. It&#39;s essentially the learning phase of the model&#39;s lifecycle. During training, the model is fed massive amounts of data. Through this process, the AI &quot;learns&quot; patterns and relationships within the training data through algorithmic adjustment of the model&#39;s parameters. This is the main workload folks are usually talking about when discussing the massive computational and energy requirements of AI.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frbwr4hbogccg14so0ebj.png\"><img alt=\"An AI model in the training phase is still learning. A robot stands poised to drop a book into a fish bowl, with books scattered haphazardly on the floor and on the bookshelf behind it. A platform engineer facepalms while a data scientist looks on with concern.\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frbwr4hbogccg14so0ebj.png\" style=\"height:533px; width:800px\" /></a></p>\r\n\r\n<p><em>During training, the AI model is fed massive amounts of data, which it &quot;learns&quot; from, algorithmically adjusting its own parameters.</em></p>\r\n\r\n<p>It&rsquo;s becoming a common strategy for teams to utilize pre-trained models instead of training their own from scratch. However, a generalized AI is often not well-equipped to handle specialized use cases. For scenarios that require a customized AI, a team is likely to do a similar &ldquo;training&rdquo; step to customize the model without fully training it. We call this &ldquo;fine-tuning.&rdquo; I&rsquo;ll dive deeper into fine-tuning strategies another time, but&nbsp;<a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-gemini-overview\">this overview of model tuning for Google Cloud&rsquo;s Gemini model</a>&nbsp;is a good resource to start with.</p>\r\n\r\n<p><strong>Why Kubernetes for Training</strong></p>\r\n\r\n<p>Kubernetes makes a lot of sense as a platform for AI training workloads. As a distributed system, Kubernetes is designed to manage a huge amount of distributed infrastructure and the networking challenges that come with it. Training workloads have significant hardware requirements, which Kubernetes can support with GPUs, TPUs, and other specialized hardware. The scale of a model can vary greatly- from fairly simple, to very complex and resource-intensive. Scaling is one of Kubernetes&#39; core competencies, meaning it can manage the variability of training workloads&#39; needs as well.</p>\r\n\r\n<p>Kubernetes is also very extensible, meaning it can integrate with additional useful tools, for example, for observability/monitoring massive training workloads. A whole ecosystem has emerged, full of useful tools for AI/Batch/HPC workloads on Kubernetes.<a href=\"https://kueue.sigs.k8s.io/\">&nbsp;Kueue</a>&nbsp;is one such tool- a Kubernetes-native open source project for managing the queueing of batch workloads on Kubernetes. To learn more about batch workloads on Kubernetes with Kueue, you might check out&nbsp;<a href=\"https://cloud.google.com/kubernetes-engine/docs/tutorials/kueue-intro\">this tutorial</a>. You can also learn more about running batch workloads on Kubernetes with GPUs in&nbsp;<a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/provisioningrequest\">this guide about running them on GKE in Google Cloud</a>.</p>\r\n\r\n<p><strong>Inference Workloads</strong></p>\r\n\r\n<p>You could say that training makes the AI into as much of an &quot;expert&quot; as it&#39;s going to be. Running a pre-trained model is its own type of workload. These &quot;inference&quot; workloads are generally much less resource-intensive than &quot;training&quot; workloads, but the resource needs of inference workloads can vary significantly. IBM defines &ldquo;AI Inferencing as: &quot;the process of running live data through a trained AI model to make a prediction or solve a task.&quot;</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy62s4n7p960vvep5vvl2.png\"><img alt=\"A skilled robot shows off its work: a perfectly organized bookshelf. The data scientist and platform engineer who created it express their approval with warm expressions, thumbs up, and clapping.\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy62s4n7p960vvep5vvl2.png\" style=\"height:600px; width:800px\" /></a></p>\r\n\r\n<p><em>An &quot;Inference Workload&quot; describes running a trained model. This model should be able to do its expected tasks relatively well.</em></p>\r\n\r\n<p>Inference workloads can range from a fairly simple, lightweight implementation - to much more complex and resource-intensive ones. The term &quot;inference workload&quot; can describe a standalone, actively running implementation of a pre-trained AI model. Or, it can describe an AI model that functions essentially as a backend service within a larger application, often in a microservice-style architecture. This term seems to be used as a catch-all for any case where a trained AI model is being run. I&rsquo;ve heard it used interchangeably with the terms &ldquo;serving workload&rdquo; and &ldquo;prediction workload.&rdquo;</p>\r\n\r\n<p><strong>Why Kubernetes for Inference</strong></p>\r\n\r\n<p>Inference workloads can have diverse resource needs. Some might be lightweight and run on CPUs, while others might require powerful GPUs for maximum performance. Kubernetes excels at managing heterogeneous hardware, allowing you to assign the right resources to each inference workload for optimal efficiency.</p>\r\n\r\n<p>Kubernetes provides flexibility in the way an inference workload is used. Users may interact with it directly as a standalone application. Or they may go through a separate frontend as part of a microservice. Whether the workload is standalone or one part of a whole, we call the workload that runs an AI model, &quot;inference.&quot;</p>\r\n\r\n<h3><strong>On Terminology</strong></h3>\r\n\r\n<p>In writing this blog post, I learned that the terminology of AI workloads is still actively being determined. &ldquo;Inference&rdquo; is currently used interchangeably with &ldquo;serving,&rdquo; &ldquo;prediction,&rdquo; and maybe more. Words have meaning, but when meanings are still being settled, it&rsquo;s especially important to be as clear as possible about what you mean when you use a term.</p>\r\n\r\n<p>One area which I think is not well-served by existing AI terminology is the difference between running a model, and running a full AI application. I have also seen the &ldquo;inference&rdquo; family of terms used to describe not just the running model, but the full application it is a part of, when the speaker is focusing on the AI aspects of that application.</p>\r\n\r\n<p>It&rsquo;s worth noting that Kubernetes is good not just for running the AI part, but also for running the application that AI model is part of, as well as related services. Serving Frameworks like Ray are useful tools for managing not just AI models, but the applications around them. I&rsquo;ll likely dive deeper into Ray in a future blog post. If you&rsquo;d like to learn more about Ray, you might check out&nbsp;<a href=\"https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay\">this blog post about Ray on GKE</a>.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsay5xd07a9grala5n87x.png\"><img alt=\"A trained libriarian bot stands welcomingly in front of an organized bookshelf. A user requests a specific book.\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsay5xd07a9grala5n87x.png\" style=\"height:800px; width:800px\" /></a></p>\r\n\r\n<p><em>AI models often fill a role within a larger application that serves users.</em></p>\r\n\r\n<p>Ultimately, be careful when you talk about AI workloads! Try to explain what you mean as clearly as you can so we can all learn and understand together!</p>\r\n\r\n<h3><strong>Customizing AI: It&#39;s All About Context</strong></h3>\r\n\r\n<p>I&#39;m enjoying learning about the ways &quot;AI&quot; fits into the world of &quot;Kubernetes,&quot; and there&#39;s a lot more to learn! In this post, we explored AI training, inference, and serving workloads and why to run them on Kubernetes. These workload types are great for understanding what it means to run AI models on Kubernetes. But the real value of AI is in its ability to understand and convey information in-context. To make a generic AI model useful in many use cases, it needs to be made aware of the context it&#39;s operating in, and what role it&#39;s fulfilling. &quot;Fine-tuning&quot; refers to the techniques for customizing a generic model, often by partially retraining it. There are also other techniques like RAG and prompt engineering that can be used to customize a generic model&rsquo;s responses without altering the model itself. I&rsquo;ll dive deeper into these techniques in a future blog post.</p>\r\n','2024-05-14','17:52:36'),(20,'A Guide to Connect Machine Learning Model Backend to Frontend Using Flask','<h2>Introduction</h2>\r\n\r\n<p>Many of us work with machine learning models for various purposes, often using&nbsp;<em>Jupyter Notebook</em>,&nbsp;<em>Google Collab</em>&nbsp;or other environments. Now we may think about how can we save our trained model and make predictions based on user input directly from our website or app. Many different methods are available out there for various purposes. However, in this article, I will show a simple approach to saving a trained model as a file and using this model file to predict actual input data from users and show prediction results.<br />\r\nHere, I have trained a basic&nbsp;<em>Linear Regression</em>&nbsp;model from the&nbsp;<em>scikit-learn</em>&nbsp;library using the&nbsp;<em>mtcars</em>&nbsp;dataset to predict&nbsp;<em>Miles Per Gallon (mpg)</em>&nbsp;based on&nbsp;<em>Horsepower (hp)</em>&nbsp;and&nbsp;<em>Weight (wt)</em>. Then, I also created a simple web app using&nbsp;<em>Flask</em>,&nbsp;<em>JQuery</em>, and&nbsp;<em>Bootstrap 5</em>&nbsp;to collect, process user input and generate predictions. You don&#39;t need to worry about the technical details, you can easily try it out by cloning/downloading my&nbsp;<em>Github</em>&nbsp;repository.</p>\r\n\r\n<p>All the resource files can be found on this&nbsp;<em>GitHub</em>&nbsp;Repository:<br />\r\n<a href=\"https://github.com/alfa-echo-niner-ait/ml-predict-app\">github.com/alfa-echo-niner-ait/ml-predict-app</a></p>\r\n\r\n<p>I have also hosted our sample web app online. Feel free to try it out at this link:<br />\r\n<a href=\"https://alfaecho.pythonanywhere.com/\">Simple Flask Web App to Predict From User Input</a></p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fovus8chn0amo3lib4evv.png\"><img alt=\"Simple Flask Web App to Predict From User Input\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fovus8chn0amo3lib4evv.png\" style=\"height:330px; width:800px\" /></a></p>\r\n\r\n<h2>Requirements</h2>\r\n\r\n<p>To try out this tutorial you need to have&nbsp;<em>Python</em>&nbsp;installed on your local machine and a text editor or&nbsp;<em>IDE</em>&nbsp;that supports&nbsp;<em>Python</em>&nbsp;and editing notebook_(.ipynb)_ files. Here, I will be using my all-time favourite&nbsp;<em>Visual Studio Code</em>&nbsp;as I can just install certain extensions to continue my work.<br />\r\nNow you can either clone my repository or directly download the source code as a zip file from&nbsp;<em>GitHub</em>&nbsp;and extract it on your file storage. To clone the repository, create a new folder, then open the terminal from that folder and run the git clone command.</p>\r\n\r\n<p><code>git clone https://github.com/alfa-echo-niner-ait/ml-predict-app.git</code></p>\r\n\r\n<p>After cloning/downloading the source codes, you need to install the necessary packages. Here mainly we will need the following</p>\r\n\r\n<ul>\r\n	<li>packages:</li>\r\n	<li>flask</li>\r\n	<li>pandas</li>\r\n	<li>scikit-learn</li>\r\n	<li>joblib</li>\r\n</ul>\r\n\r\n<p>Or you can just run the following command in your resource folder where you have cloned/downloaded the source files:</p>\r\n\r\n<p><code>pip install -r requirements.txt</code></p>\r\n\r\n<p>However, it is a good practice to create a virtual environment first when we are working on big projects. Our project is small enough, we are good to go now.</p>\r\n\r\n<h2>Processing</h2>\r\n\r\n<h3>Train Model</h3>\r\n\r\n<p>For this tutorial, I&#39;ve used a simple car dataset. Here is the overview of the dataset:</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsrffkubp8vy13eyokfs5.png\"><img alt=\"Dataset Overview\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsrffkubp8vy13eyokfs5.png\" style=\"height:207px; width:800px\" /></a></p>\r\n\r\n<p>We will be predicting&nbsp;<em>mpg</em>&nbsp;based on&nbsp;<em>hp</em>&nbsp;and&nbsp;<em>wt</em>. So, we need these three columns from our dataset. So, I will assign them accordingly and split them for training and testing data.</p>\r\n\r\n<pre>\r\n<code>X = data[[&#39;hp&#39;, &#39;wt&#39;]]\r\nY = data[&#39;mpg&#39;]\r\n\r\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=11)\r\n</code></pre>\r\n\r\n<blockquote>\r\n<p>If you are willing to use your models to predict user input data, pay attention to the training process, especially picking the right columns. Picking an unnecessary or extensive amount of columns affects the model&#39;s performance. Make sure which columns you really need and exclude the remaining. Also, choose the correct model according to your dataset.</p>\r\n</blockquote>\r\n\r\n<p>Here I have used&nbsp;<em>LinearRegression</em>&nbsp;model from the scikit-learn library to train and test.</p>\r\n\r\n<pre>\r\n<code>from sklearn.linear_model import LinearRegression\r\nlr_model = LinearRegression()\r\n</code></pre>\r\n\r\n<p>You can check out the full model training and testing process that has been included with the resource files in the&nbsp;<em>/Model/Model_train.ipynb</em></p>\r\n\r\n<h3>Save Model</h3>\r\n\r\n<p>After we finish training, testing, and evaluating the accuracy and other necessary metrics, and are satisfied with our model performance, we are ready to save our trained model. My model has achieved an accuracy rate of&nbsp;<em>88.48%</em>, which is good enough.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwdpji7mdbv2e4wdobblq.png\"><img alt=\"Model Accuracy\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwdpji7mdbv2e4wdobblq.png\" style=\"height:272px; width:666px\" /></a></p>\r\n\r\n<p>Here, we have used the&nbsp;<em>joblib</em>&nbsp;library to save our model, by&nbsp;<em>joblib.dump()</em>&nbsp;function. To save the model, you have to pass your training model (in our case&nbsp;<em>lr_model</em>) and filename with an extension. Then, our model will be saved in the root directory of our notebook file with the mentioned filename(model_filename).</p>\r\n\r\n<blockquote>\r\n<p>In model_filename if the file extension is not mentioned, filename extensions (&#39;.z&#39;, &#39;.gz&#39;, &#39;.bz2&#39;, &#39;.xz&#39; or &#39;.lzma&#39;) will be used automatically.</p>\r\n</blockquote>\r\n\r\n<pre>\r\n<code>import joblib\r\n\r\n# Define the save model name\r\nmodel_filename = &#39;car_model.joblib&#39;\r\n# Save the model\r\njoblib.dump(lr_model, model_filename)\r\n</code></pre>\r\n\r\n<p>The&nbsp;<em>joblib.dump()</em>&nbsp;function can receive the following attributes:</p>\r\n\r\n<pre>\r\n<code>joblib.dump(value, filename, compress=0, protocol=None, cache_size=None)\r\n</code></pre>\r\n\r\n<p><em>protocol</em>&nbsp;and&nbsp;<em>cache_size</em>&nbsp;are optional. For compress, it supports int from 0 to 9 or bool or 2-tuple, or you can also leave it as optional.</p>\r\n\r\n<h3>Load Model</h3>\r\n\r\n<p>Now we have saved our model, it&#39;s time to load the model and do the prediction. We can load our saved model simply by using&nbsp;<em>joblib.load()</em>&nbsp;function.</p>\r\n\r\n<pre>\r\n<code>loaded_model = joblib.load(model_filename)\r\n</code></pre>\r\n\r\n<p>This function has two attributes,&nbsp;<em>filename(str, pathlib.Path, or file object)</em>&nbsp;and&nbsp;<em>mmap_mode({None, &#39;r+&#39;, &#39;r&#39;, &#39;w+&#39;, &#39;c&#39;})</em>&nbsp;or you can leave&nbsp;<em>mmap_mode</em>&nbsp;as optional.</p>\r\n\r\n<blockquote>\r\n<p>WARNING: joblib.load relies on the pickle module and can therefore execute arbitrary Python code. It should therefore never be used to load files from untrusted sources.</p>\r\n</blockquote>\r\n\r\n<pre>\r\n<code>joblib.load(filename, mmap_mode=None)\r\n</code></pre>\r\n\r\n<h3>Predict With Loaded&nbsp;Model</h3>\r\n\r\n<p>To predict using your loaded model you have to pass the data with the exact format of your training data. You can check the training and testing data format by printing their type or check your training process to find your format.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flsviopuk1249frxirq4b.png\"><img alt=\"Check Data Type\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flsviopuk1249frxirq4b.png\" style=\"height:271px; width:541px\" /></a></p>\r\n\r\n<p>For example, here we have hp and wt data separately, then we convert them to&nbsp;<em>pandas.core.frame.DataFrame</em>, and then use for prediction.</p>\r\n\r\n<pre>\r\n<code># Input data\r\nhp = 110\r\nwt = 3.2\r\n\r\n# Convert accordingly\r\ndata = {\r\n    &quot;hp&quot;: [hp],\r\n    &quot;wt&quot;: [wt]\r\n}\r\ntest_data = pd.DataFrame(data)\r\n\r\n# Predict\r\npredicted_mpg = loaded_model.predict(test_data)\r\n\r\n# Print Result\r\nprint(f&quot;Result: {predicted_mpg[0]:.2f} mpg&quot;)\r\n</code></pre>\r\n\r\n<h2>Working with&nbsp;Flask</h2>\r\n\r\n<h3>Load Model in the&nbsp;App</h3>\r\n\r\n<p>If you have already cloned/downloaded the source code from my&nbsp;<em>GitHub</em>&nbsp;repository, you might&#39;ve noticed that we load our model file when we initialize our app in&nbsp;<em>/source/</em><em>init</em><em>.py</em>, it will prevent reloading the model again and again. In the&nbsp;<em>MODEL_NAME</em>&nbsp;mention your saved model filename. Here, I have placed my model file inside&nbsp;<em>/source/static/model</em>&nbsp;folder. If you place your model in a different folder, remember to modify the&nbsp;<em>model_path</em>.</p>\r\n\r\n<pre>\r\n<code>import os\r\nimport joblib\r\nfrom flask import Flask\r\n\r\nMODEL_NAME = &quot;car_model.joblib&quot;\r\n\r\n# Initialize the app\r\napp = Flask(__name__)\r\n\r\n# Load the model when the application starts\r\nmodel_path = os.path.join(os.path.dirname(__file__), &#39;static/model/&#39; + MODEL_NAME)\r\nmodel = joblib.load(model_path)\r\n</code></pre>\r\n\r\n<h3>User Input</h3>\r\n\r\n<p>In our&nbsp;<em>/source/templates/index.html</em>, we have a simple form to take hp and wt input from the user.</p>\r\n\r\n<pre>\r\n<code>&lt;!-- User Input --&gt;\r\n&lt;input type=&quot;text&quot; id=&quot;hp&quot; class=&quot;form-control-lg&quot; placeholder=&quot;Enter HP, e.g. 112&quot; required&gt;\r\n&lt;input type=&quot;text&quot; id=&quot;wt&quot; class=&quot;form-control-lg&quot; placeholder=&quot;Enter WT, e.g. 3.2&quot; required&gt;\r\n&lt;!-- Process input/output via main.js file --&gt;\r\n &lt;button class=&quot;btn btn-lg btn-info&quot; type=&quot;button&quot; id=&quot;submitData&quot;&gt;Get MPG&lt;/button&gt;\r\n</code></pre>\r\n\r\n<h3>Send Data to Flask&nbsp;Server</h3>\r\n\r\n<p>When the user submits the data we read the form data, send it to our flask server and get a response using&nbsp;<em>AJAX</em>&nbsp;request. The code can be found inside our&nbsp;<em>/source/static/js/main.js</em>&nbsp;file. However, you can use any other method you like to exchange request/response data with your server. Here&#39;s how I did it:</p>\r\n\r\n<pre>\r\n<code>$(document).ready(function () {\r\n    // Send data to flask server upon submit\r\n    $(&quot;#submitData&quot;).click(function () {\r\n        $(&quot;#resText&quot;).show()\r\n        $(&quot;#resText&quot;).html(&quot;Loading....&quot;);\r\n        hp = $(&quot;#hp&quot;).val();\r\n        wt = $(&quot;#wt&quot;).val();\r\n        $(&quot;#hp&quot;).val(&quot;&quot;);\r\n        $(&quot;#wt&quot;).val(&quot;&quot;);\r\n\r\n        // Data request\r\n        $.ajax({\r\n            type: &quot;POST&quot;,\r\n            url: predict_url,\r\n            data: {\r\n                hp: hp,\r\n                wt: wt\r\n            },\r\n            // Print result\r\n            success: function (response) {\r\n                $(&quot;#resText&quot;).html(response);\r\n            }\r\n        });\r\n    });\r\n});\r\n</code></pre>\r\n\r\n<h3>Process, Predict and Return&nbsp;Result</h3>\r\n\r\n<p>We have defined a&nbsp;<em>/predict</em>&nbsp;route in&nbsp;<em>/source/routes.py</em>&nbsp;flask app to read the data from the&nbsp;<em>AJAX</em>&nbsp;request, then process the data, predict and return the result.</p>\r\n\r\n<pre>\r\n<code># Process user input and return response data\r\n@app.route(&#39;/predict&#39;, methods=[&#39;POST&#39;])\r\ndef predict():\r\n    if request.method == &#39;POST&#39; and model:\r\n        # Receive &amp; store the form input data by user\r\n        hp = request.form.get(&#39;hp&#39;)\r\n        wt = request.form.get(&#39;wt&#39;)\r\n\r\n        # Prepare received data for test data according to your trained model\r\n        # Note: Prepare your user input as per your trained model prediction data\r\n        data = {\r\n            &quot;hp&quot;: [hp],\r\n            &quot;wt&quot;: [wt]\r\n        }\r\n        test_data = pd.DataFrame(data)\r\n        try:\r\n            # Calculate the prediction result as per your trained model\r\n            predict_mpg = model.predict(test_data)\r\n            result = f&quot;{predict_mpg[0]:.2f}&quot;\r\n            return f&quot;Predicted : &lt;strong&gt;{result} MPG&lt;/strong&gt;&quot;\r\n        except ValueError:\r\n            return &quot;&lt;i class=&#39;text-danger&#39;&gt;Please Enter Correct Data!&lt;/i&gt;&quot;\r\n\r\n    return &quot;Failed to load model!&quot;\r\n</code></pre>\r\n\r\n<p>Here we process the user data the same way we showed in the&nbsp;<strong>&quot;Predict With Loaded Model&quot;</strong>&nbsp;section.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>With this tutorial, I have tried to demonstrate some basic ideas for implementing a machine-learning model to interact with users. However, these ideas are not limited, you can try out more with different models and different frameworks like Django, FastAPI etc. or more complex applications to not just predict user input but also concurrently feed the model live user data and predict various aspects to improve user experience and provide better services.</p>\r\n\r\n<p>And you&#39;re welcome to explore the internet for more ideas on this topic and build a strong skillset. I have attached all the links to the Reference section of the technology I&#39;ve discussed through my tutorial.</p>\r\n\r\n<h2>Reference Links</h2>\r\n\r\n<ul>\r\n	<li><a href=\"https://www.python.org/\">Python</a></li>\r\n	<li><a href=\"https://www.python.org/\">Flask</a></li>\r\n	<li><a href=\"https://scikit-learn.org/\">scikit-learn</a></li>\r\n	<li><a href=\"https://joblib.readthedocs.io/\">joblib</a></li>\r\n	<li><a href=\"https://joblib.readthedocs.io/\">pandas</a></li>\r\n	<li><a href=\"https://getbootstrap.com/\">Bootstrap</a></li>\r\n	<li><a href=\"https://getbootstrap.com/\">jQuery</a></li>\r\n	<li><a href=\"https://getbootstrap.com/\">Jupyter</a></li>\r\n	<li><a href=\"https://colab.research.google.com/\">Google Colaboratory</a></li>\r\n	<li><a href=\"https://code.visualstudio.com/\">VS Code</a></li>\r\n	<li><a href=\"https://www.djangoproject.com/\">Django</a></li>\r\n	<li><a href=\"https://fastapi.tiangolo.com/\">FastAPI</a></li>\r\n</ul>\r\n','2024-05-14','17:53:26'),(21,'AI-Powered Code Documentation and Analysis','<h2><strong>TL;DR</strong></h2>\r\n\r\n<p>In this article, you will learn how to use&nbsp;<a href=\"https://www.mimrr.com/\">Mimrr</a>, an AI tool to generate documentation for your code. Also, you will learn how to use the Mimrr to analyze your code for:</p>\r\n\r\n<ul>\r\n	<li>Bugs</li>\r\n	<li>Maintainability Issues</li>\r\n	<li>Performance Issues</li>\r\n	<li>Security Issues</li>\r\n	<li>Optimization Issues</li>\r\n</ul>\r\n\r\n<p>Leveraging the power of&nbsp;<a href=\"https://www.mimrr.com/\">Mimrr</a>&nbsp;code documentation and analytics will enable you to create, and maintain up-to-date code documentation even when there are regular code changes.</p>\r\n\r\n<p>In return, you will save a lot of time that could have been spent in writing and maintaining your code documentation manually.</p>\r\n\r\n<h2>Getting Started With Mimrr</h2>\r\n\r\n<p>In this section, you will learn how to create a Mimrr account.</p>\r\n\r\n<p><strong>Step 1:</strong>&nbsp;Go to&nbsp;<a href=\"https://www.mimrr.com/\">Mimrr</a>&nbsp;and click the Start Free button.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5rwo8j14i10jwjz5szme.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5rwo8j14i10jwjz5szme.png\" style=\"height:404px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 2:</strong>&nbsp;Then create your Mimrr account using your Google, Microsoft, or GitHub account.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fobxuktfuoehkqqc6mlxv.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fobxuktfuoehkqqc6mlxv.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 3:</strong>&nbsp;Next, create an organization by adding an organization name and its description. Then click the Create Organization button, as shown below.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft1f53xqglv3gc178vlwp.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft1f53xqglv3gc178vlwp.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p>After that, you will be redirected to your Mimrr dashboard to connect the codebase repo that you want to generate documentation for.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi3syu9rdwxez69j243h9.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi3syu9rdwxez69j243h9.png\" style=\"height:404px; width:800px\" /></a></p>\r\n\r\n<p>Congratulations! You have successfully created a Mimrr account.</p>\r\n\r\n<h2>Connecting Your Codebase Repo To Mimrr To Generate Code Documentation</h2>\r\n\r\n<p>In this section, you will learn how to connect your codebase GitHub repo to Mimrr to generate its documentation and analytics.</p>\r\n\r\n<p><strong>Step 1:</strong>&nbsp;Go to the dashboard and open the Connect your code to Mimrr drop-down menu. Then click the Connect button.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fnhmbktuaam4y6qrxibjz.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fnhmbktuaam4y6qrxibjz.png\" style=\"height:413px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 2:</strong>&nbsp;Then you will be redirected to choose a repository provider. In this case, I will select GitHub as my code provider.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F98ssmd5fo0eh44m6orx0.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F98ssmd5fo0eh44m6orx0.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 3:</strong>&nbsp;Next, go to your Mimrr dashboard and open the projects section to add your codebase repository by clicking the Add Project button. Once your project is added, it should look as shown below.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fntebvnv1jinklznfmr5x.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fntebvnv1jinklznfmr5x.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 4:</strong>&nbsp;Click on the project to view the generated documentation, as shown below.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4okq15fsh9vi4q11kvkl.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4okq15fsh9vi4q11kvkl.png\" style=\"height:402px; width:800px\" /></a></p>\r\n\r\n<p>Congratulations! You have successfully generated code documentation for your codebase.</p>\r\n\r\n<h2>Viewing Code Analytics</h2>\r\n\r\n<p>In this section, you will learn how to view the analytics of your code that highlight Bugs, Smells, Performance, Security, and Refactoring Issues.</p>\r\n\r\n<p><strong>Step 1:</strong>&nbsp;On your Mimrr dashboard, go to your selected project and open the Analytics section. You will then get an overview of your codebase health, as shown below.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F71mpjra3skzi7c8i5cs5.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F71mpjra3skzi7c8i5cs5.png\" style=\"height:404px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 2:</strong>&nbsp;Click one of the summarised sections. Then click the Details drop-down menu to see more details about the issue, as shown below.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fa5mbzq2804b0avcu631s.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fa5mbzq2804b0avcu631s.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 3:</strong>&nbsp;To analyze your code maintainability issues, select Code Smell on the drop-down menu, as shown below. Then check more details for each maintainability issue.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbmxp6y6mptr20rr2g617.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbmxp6y6mptr20rr2g617.png\" style=\"height:403px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 4:</strong>&nbsp;To analyze your code performance issues, select Performance on the drop-down menu, as shown below. Then check more details for each performance issue.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzjf820vkaooddb086ger.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzjf820vkaooddb086ger.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 5:</strong>&nbsp;To analyze your code security issues, select Security on the drop-down menu, as shown below. Then check more details for each vulnerability issue.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F290x3vo59u3oq4a8g3vb.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F290x3vo59u3oq4a8g3vb.png\" style=\"height:405px; width:800px\" /></a></p>\r\n\r\n<p><strong>Step 6:</strong>&nbsp;To analyze your code refactoring issues, select Refactor on the drop-down menu, as shown below. Then check more details for each flexibility issue.</p>\r\n\r\n<p><a href=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0lhe92fncuisq61rspkv.png\"><img alt=\"Image description\" src=\"https://media.dev.to/cdn-cgi/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0lhe92fncuisq61rspkv.png\" style=\"height:404px; width:800px\" /></a></p>\r\n\r\n<p>Congratulations! You have successfully analyzed your code for Bugs, Smells, Performance, Security, and Refactoring Issues.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>In conclusion, Mimrr is a great tool for automating the process of generating code documentation and analyzing any issues with your code. By using Mimrr, you will be able to ship features faster without technical debt.</p>\r\n','2024-05-14','23:25:10');
/*!40000 ALTER TABLE `post_data` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts`
--

DROP TABLE IF EXISTS `posts`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `posts` (
  `post_id` int NOT NULL AUTO_INCREMENT,
  `post_user_id` int NOT NULL,
  PRIMARY KEY (`post_id`),
  KEY `post_user_id_idx` (`post_user_id`),
  CONSTRAINT `post_user_id` FOREIGN KEY (`post_user_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=22 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts`
--

LOCK TABLES `posts` WRITE;
/*!40000 ALTER TABLE `posts` DISABLE KEYS */;
INSERT INTO `posts` VALUES (9,13),(10,13),(20,13),(11,14),(12,14),(13,14),(19,15),(21,16),(15,17),(14,18),(18,19),(16,20);
/*!40000 ALTER TABLE `posts` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `users`
--

DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `users` (
  `id` int NOT NULL AUTO_INCREMENT,
  `username` varchar(45) DEFAULT NULL,
  `password_hash` varchar(200) NOT NULL,
  `email` varchar(45) NOT NULL,
  `role` varchar(45) NOT NULL,
  `points` int NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=22 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `users`
--

LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES (12,'Puna','$2b$12$oTt3dD22kvmvdEWnSWgL6.fdAffj9v4TsvcyiS7aOjJbCnhj20.5O','phphonesamay@gmail.com','Admin',10),(13,'Emon','$2b$12$c/PuOLaUvyJiVDACgu1eQugtt5mbJJbFH9PISEW1yp0Iuu9TGwL3W','m.aaemon98@gmail.com','Developer',202),(14,'Tooktick','$2b$12$HLM/7/zsC5cZjVgVVFZOduTJbfK6Y2mw8QBE0.MNYVLW.BEZgFnna','tooktick@gmail.com','Learner',42),(15,'Bill','$2b$12$bKSXyeG796rkB7o71HAMneD0.zWaL2sYievwjKZ9A8gRD0ezjlUOi','bill@163.com','Learner',20),(16,'Namfon','$2b$12$hSE2crbuXWIVAJs15POv.uzn/8amvSYbb.7u36gRtLPhGgum1ZxNu','namfon@163.com','Learner',20),(17,'Bella','$2b$12$iqU7OrGLNjH8jS04TpRnPeyZDX54QFgCsW6umswOkGshiPwn2H.3a','bella@163.com','Learner',20),(18,'美丽','$2b$12$7BbvQekfD3aPHkeR8cnUoOs0LTsQafSaWsWQRsFUEu3EzjTYsRq/u','meili@163.com','Learner',20),(19,'Seenve','$2b$12$aa7Y5rwoQ8Ed1kza3//4auy0TRoFrzn50K6iwT2W79B8C4TieOp9e','seenve@163.com','Learner',30),(20,'Sou','$2b$12$ylEB7zwS9VMjxiFUILVlHud0uU4qcFVTcvJUM.B6jbjtKNi0zD10S','sou@163.com','Learner',20);
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `users_info`
--

DROP TABLE IF EXISTS `users_info`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `users_info` (
  `i_id` int NOT NULL,
  `first_name` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,
  `last_name` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,
  `gender` varchar(10) NOT NULL,
  `birthdate` date DEFAULT NULL,
  `address` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL,
  `profile_img` varchar(100) DEFAULT 'default.png',
  PRIMARY KEY (`i_id`),
  CONSTRAINT `i_id` FOREIGN KEY (`i_id`) REFERENCES `users` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `users_info`
--

LOCK TABLES `users_info` WRITE;
/*!40000 ALTER TABLE `users_info` DISABLE KEYS */;
INSERT INTO `users_info` VALUES (12,'Phonesamay','Phoutthavong','Female','2002-12-12','Vientiane Road, Thakhek City, Khammouane Province, Laos','default.png'),(13,'Ayub Ali','Emon','Male','1998-08-27','Qingdao city, Shandong province, Republic of China','e807c987492eef4a.JPG'),(14,'Thipphasone','Phoutthavong','Female',NULL,'Vientiane, Laos','df0b8ab617c7a0e5.png'),(15,'Inthathaksin','Sithimorada','Male',NULL,'','540fca2cdc0e0f80.JPG'),(16,'Soukthaxay','Phoutthavong','Female',NULL,'','1dd39427262a62b6.JPG'),(17,'Feng','Naidan','Female',NULL,'','0fb100b90b252fa2.jpg'),(18,'丽','美','Female',NULL,NULL,'default.png'),(19,'เวธนี ','ธนู','Female',NULL,NULL,'default.png'),(20,'ສຸວັນນີ','ວົງສາແກ້ວ','Female',NULL,NULL,'default.png');
/*!40000 ALTER TABLE `users_info` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Dumping events for database 'mlcol'
--

--
-- Dumping routines for database 'mlcol'
--
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2024-05-16 23:30:55
